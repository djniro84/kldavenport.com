<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.54.0" />
  <link rel="canonical" href="https://kldavenport.netlify.com/a-wild-dataset-has-appeared/">

  

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://kldavenport.netlify.com/css/prism.css" media="none" onload="this.media='all';">

  
  
  <link rel="stylesheet" type="text/css" href="https://kldavenport.netlify.com/css/styles.css">

  <style id="inverter" media="none">
    html { filter: invert(100%) }
    * { background-color: inherit }
    img:not([src*=".svg"]), .colors, iframe, .demo-container { filter: invert(100%) }
  </style>

  
  
  <title>A wild dataset has appeared! Now what? | Kevin Davenport Engineering &amp; ML blog</title>
</head>

  <body>
    <a href="#main">skip to content</a>
    <svg style="display: none">
  <symbol id="bookmark" viewBox="0 0 40 50">
   <g transform="translate(2266 3206.2)">
    <path style="stroke:currentColor;stroke-width:3.2637;fill:none" d="m-2262.2-3203.4-.2331 42.195 16.319-16.318 16.318 16.318.2331-42.428z"/>
   </g>
  </symbol>

  <symbol id="w3c" viewBox="0 0 127.09899 67.763">
   <text font-size="83" style="font-size:83px;font-family:Trebuchet;letter-spacing:-12;fill-opacity:0" letter-spacing="-12" y="67.609352" x="-26.782778">W3C</text>
   <text font-size="83" style="font-size:83px;font-weight:bold;font-family:Trebuchet;fill-opacity:0" y="67.609352" x="153.21722" font-weight="bold">SVG</text>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m33.695.377 12.062 41.016 12.067-41.016h8.731l-19.968 67.386h-.831l-12.48-41.759-12.479 41.759h-.832l-19.965-67.386h8.736l12.061 41.016 8.154-27.618-3.993-13.397h8.737z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m91.355 46.132c0 6.104-1.624 11.234-4.862 15.394-3.248 4.158-7.45 6.237-12.607 6.237-3.882 0-7.263-1.238-10.148-3.702-2.885-2.47-5.02-5.812-6.406-10.022l6.82-2.829c1.001 2.552 2.317 4.562 3.953 6.028 1.636 1.469 3.56 2.207 5.781 2.207 2.329 0 4.3-1.306 5.909-3.911 1.609-2.606 2.411-5.738 2.411-9.401 0-4.049-.861-7.179-2.582-9.399-1.995-2.604-5.129-3.912-9.397-3.912h-3.327v-3.991l11.646-20.133h-14.062l-3.911 6.655h-2.493v-14.976h32.441v4.075l-12.31 21.217c4.324 1.385 7.596 3.911 9.815 7.571 2.22 3.659 3.329 7.953 3.329 12.892z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.21 0 1.414 8.6-5.008 9.583s-1.924-4.064-5.117-6.314c-2.693-1.899-4.447-2.309-7.186-1.746-3.527.73-7.516 4.938-9.258 10.13-2.084 6.21-2.104 9.218-2.178 11.978-.115 4.428.58 7.043.58 7.043s-3.04-5.626-3.011-13.866c.018-5.882.947-11.218 3.666-16.479 2.404-4.627 5.954-7.404 9.114-7.728 3.264-.343 5.848 1.229 7.841 2.938 2.089 1.788 4.213 5.698 4.213 5.698l4.94-9.837z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.82 48.674s-2.208 3.957-3.589 5.48c-1.379 1.524-3.849 4.209-6.896 5.555-3.049 1.343-4.646 1.598-7.661 1.306-3.01-.29-5.807-2.032-6.786-2.764-.979-.722-3.486-2.864-4.897-4.854-1.42-2-3.634-5.995-3.634-5.995s1.233 4.001 2.007 5.699c.442.977 1.81 3.965 3.749 6.572 1.805 2.425 5.315 6.604 10.652 7.545 5.336.945 9.002-1.449 9.907-2.031.907-.578 2.819-2.178 4.032-3.475 1.264-1.351 2.459-3.079 3.116-4.108.487-.758 1.276-2.286 1.276-2.286l-1.276-6.644z"/>
  </symbol>

  <symbol id="tag" viewBox="0 0 177.16535 177.16535">
    <g transform="translate(0 -875.2)">
     <path style="fill-rule:evenodd;stroke-width:0;fill:currentColor" d="m159.9 894.3-68.79 8.5872-75.42 77.336 61.931 60.397 75.429-76.565 6.8495-69.755zm-31.412 31.835a10.813 10.813 0 0 1 1.8443 2.247 10.813 10.813 0 0 1 -3.5174 14.872l-.0445.0275a10.813 10.813 0 0 1 -14.86 -3.5714 10.813 10.813 0 0 1 3.5563 -14.863 10.813 10.813 0 0 1 13.022 1.2884z"/>
    </g>
  </symbol>

  <symbol id="balloon" viewBox="0 0 141.73228 177.16535">
   <g transform="translate(0 -875.2)">
    <g>
     <path style="fill:currentColor" d="m68.156 882.83-.88753 1.4269c-4.9564 7.9666-6.3764 17.321-5.6731 37.378.36584 10.437 1.1246 23.51 1.6874 29.062.38895 3.8372 3.8278 32.454 4.6105 38.459 4.6694-.24176 9.2946.2879 14.377 1.481 1.2359-3.2937 5.2496-13.088 8.886-21.623 6.249-14.668 8.4128-21.264 10.253-31.252 1.2464-6.7626 1.6341-12.156 1.4204-19.764-.36325-12.93-2.1234-19.487-6.9377-25.843-2.0833-2.7507-6.9865-7.6112-7.9127-7.8436-.79716-.20019-6.6946-1.0922-6.7755-1.0248-.02213.0182-5.0006-.41858-7.5248-.22808l-2.149-.22808h-3.3738z"/>
     <path style="fill:currentColor" d="m61.915 883.28-3.2484.4497c-1.7863.24724-3.5182.53481-3.8494.63994-2.4751.33811-4.7267.86957-6.7777 1.5696-.28598 0-1.0254.20146-2.3695.58589-5.0418 1.4418-6.6374 2.2604-8.2567 4.2364-6.281 7.6657-11.457 18.43-12.932 26.891-1.4667 8.4111.71353 22.583 5.0764 32.996 3.8064 9.0852 13.569 25.149 22.801 37.517 1.3741 1.841 2.1708 2.9286 2.4712 3.5792 3.5437-1.1699 6.8496-1.9336 10.082-2.3263-1.3569-5.7831-4.6968-21.86-6.8361-33.002-.92884-4.8368-2.4692-14.322-3.2452-19.991-.68557-5.0083-.77707-6.9534-.74159-15.791.04316-10.803.41822-16.162 1.5026-21.503 1.4593-5.9026 3.3494-11.077 6.3247-15.852z"/>
     <path style="fill:currentColor" d="m94.499 885.78c-.10214-.0109-.13691 0-.0907.0409.16033.13489 1.329 1.0675 2.5976 2.0723 6.7003 5.307 11.273 14.568 12.658 25.638.52519 4.1949.24765 14.361-.5059 18.523-2.4775 13.684-9.7807 32.345-20.944 53.519l-3.0559 5.7971c2.8082.76579 5.7915 1.727 8.9926 2.8441 11.562-11.691 18.349-19.678 24.129-28.394 7.8992-11.913 11.132-20.234 12.24-31.518.98442-10.02-1.5579-20.876-6.7799-28.959-.2758-.4269-.57803-.86856-.89617-1.3166-3.247-6.13-9.752-12.053-21.264-16.131-2.3687-.86369-6.3657-2.0433-7.0802-2.1166z"/>
     <path style="fill:currentColor" d="m32.52 892.22c-.20090-.13016-1.4606.81389-3.9132 2.7457-11.486 9.0476-17.632 24.186-16.078 39.61.79699 7.9138 2.4066 13.505 5.9184 20.562 5.8577 11.77 14.749 23.219 30.087 38.74.05838.059.12188.1244.18052.1838 1.3166-.5556 2.5965-1.0618 3.8429-1.5199-.66408-.32448-1.4608-1.3297-3.8116-4.4602-5.0951-6.785-8.7512-11.962-13.051-18.486-5.1379-7.7948-5.0097-7.5894-8.0586-13.054-6.2097-11.13-8.2674-17.725-8.6014-27.563-.21552-6.3494.13041-9.2733 1.775-14.987 2.1832-7.5849 3.9273-10.986 9.2693-18.07 1.7839-2.3656 2.6418-3.57 2.4409-3.7003z"/>
     <path style="fill:currentColor" d="m69.133 992.37c-6.2405.0309-12.635.76718-19.554 2.5706 4.6956 4.7759 9.935 10.258 12.05 12.625l4.1272 4.6202h11.493l3.964-4.4516c2.0962-2.3541 7.4804-7.9845 12.201-12.768-8.378-1.4975-16.207-2.6353-24.281-2.5955z"/>
     <rect style="stroke-width:0;fill:currentColor" ry="2.0328" height="27.746" width="22.766" y="1017.7" x="60.201"/>
    </g>
   </g>
  </symbol>

  <symbol id="info" viewBox="0 0 41.667 41.667">
   <g transform="translate(-37.035 -1004.6)">
    <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m76.25 1030.2a18.968 18.968 0 0 1 -23.037 13.709 18.968 18.968 0 0 1 -13.738 -23.019 18.968 18.968 0 0 1 23.001 -13.768 18.968 18.968 0 0 1 13.798 22.984"/>
    <g transform="matrix(1.1146 0 0 1.1146 -26.276 -124.92)">
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m75.491 1039.5v-8.7472"/>
     <path style="stroke-width:0;fill:currentColor" transform="scale(-1)" d="m-73.193-1024.5a2.3719 2.3719 0 0 1 -2.8807 1.7142 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
   </g>
  </symbol>

  <symbol id="warning" viewBox="0 0 48.430474 41.646302">
    <g transform="translate(-1.1273 -1010.2)">
     <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:4.151;fill:none" d="m25.343 1012.3-22.14 37.496h44.28z"/>
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:4.1512;fill:none" d="m25.54 1027.7v8.7472"/>
     <path style="stroke-width:0;fill:currentColor" d="m27.839 1042.8a2.3719 2.3719 0 0 1 -2.8807 1.7143 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
  </symbol>

  <symbol id="menu" viewBox="0 0 50 50">
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="0" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="20" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="40" x="0"/>
   </symbol>

   <symbol id="link" viewBox="0 0 50 50">
    <g transform="translate(0 -1002.4)">
     <g transform="matrix(.095670 0 0 .095670 2.3233 1004.9)">
      <g>
       <path style="stroke-width:0;fill:currentColor" d="m452.84 192.9-128.65 128.65c-35.535 35.54-93.108 35.54-128.65 0l-42.881-42.886 42.881-42.876 42.884 42.876c11.845 11.822 31.064 11.846 42.886 0l128.64-128.64c11.816-11.831 11.816-31.066 0-42.9l-42.881-42.881c-11.822-11.814-31.064-11.814-42.887 0l-45.928 45.936c-21.292-12.531-45.491-17.905-69.449-16.291l72.501-72.526c35.535-35.521 93.136-35.521 128.64 0l42.886 42.881c35.535 35.523 35.535 93.141-.001 128.66zm-254.28 168.51-45.903 45.9c-11.845 11.846-31.064 11.817-42.881 0l-42.884-42.881c-11.845-11.821-11.845-31.041 0-42.886l128.65-128.65c11.819-11.814 31.069-11.814 42.884 0l42.886 42.886 42.876-42.886-42.876-42.881c-35.54-35.521-93.113-35.521-128.65 0l-128.65 128.64c-35.538 35.545-35.538 93.146 0 128.65l42.883 42.882c35.51 35.54 93.11 35.54 128.65 0l72.496-72.499c-23.956 1.597-48.092-3.784-69.474-16.283z"/>
      </g>
     </g>
    </g>
  </symbol>

  <symbol id="doc" viewBox="0 0 35 45">
   <g transform="translate(-147.53 -539.83)">
    <path style="stroke:currentColor;stroke-width:2.4501;fill:none" d="m149.38 542.67v39.194h31.354v-39.194z"/>
    <g style="stroke-width:25" transform="matrix(.098003 0 0 .098003 133.69 525.96)">
     <path d="m220 252.36h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path style="stroke:currentColor;stroke-width:25;fill:none" d="m220 409.95h200"/>
     <path d="m220 488.74h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path d="m220 331.15h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
    </g>
   </g>
 </symbol>

 <symbol id="tick" viewBox="0 0 177.16535 177.16535">
  <g transform="translate(0 -875.2)">
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="155" width="40" y="702.99" x="556.82"/>
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="40" width="90.404" y="817.99" x="506.42"/>
  </g>
 </symbol>
</svg>

    <div class="wrapper">
      <header class="intro-and-nav" role="banner">
  <div>
    <div class="intro">
      <a class="logo" href="/" aria-label="Kevin Davenport Engineering &amp; ML blog home page">
        <img src="https://kldavenport.netlify.com/images/logo.svg" alt="">
      </a>
      <p class="library-desc">
        
      </p>
    </div>
    <nav id="patterns-nav" class="patterns" role="navigation">
  <h2 class="vh">Main navigation</h2>
  <button id="menu-button" aria-expanded="false">
    <svg viewBox="0 0 50 50" aria-hidden="true" focusable="false">
      <use xlink:href="#menu"></use>
    </svg>
    Menu
  </button>
  
  <ul id="patterns-list">
  
    <li class="pattern">
      
      
      
      
      <a href="/post/" aria-current="page">
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Blog</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/about/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">About</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/bike-travel/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Bike Travel</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/tags/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Tags</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/index.xml" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">RSS</span>
      </a>
    </li>
  
  </ul>
</nav>
  </div>
</header>
      <div class="main-and-footer">
        <div>
          
  <main id="main">
    <h1>
      <svg class="bookmark-icon" aria-hidden="true" viewBox="0 0 40 50" focusable="false">
        <use xlink:href="#bookmark"></use>
      </svg>
      A wild dataset has appeared! Now what?
    </h1>
    <div class="date">
      
      <strong aria-hidden="true">Publish date: </strong>Mar 17, 2016
    </div>
    
      <div class="tags">
        <strong aria-hidden="true">Tags: </strong>
        <ul aria-label="tags">
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/data-science">data science</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/data-exploration">data exploration</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/data-engineering">data engineering</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/correlation">correlation</a>
            </li>
          
        </ul>
      </div>
    
    
    
      

  <nav class="toc" aria-labelledby="toc-heading">
    <h2 id="toc-heading">Table of contents</h2>
    <ol>
      
        <li>
          <a href="#our-example-data">
            Our Example Data
          </a>
        </li>
      
        <li>
          <a href="#exploratory-analysis">
            Exploratory Analysis
          </a>
        </li>
      
        <li>
          <a href="#feature-target-distributions">
            Feature-Target Distributions
          </a>
        </li>
      
        <li>
          <a href="#correlation-structure">
            Correlation Structure
          </a>
        </li>
      
        <li>
          <a href="#dimensional-reduction">
            Dimensional Reduction
          </a>
        </li>
      
        <li>
          <a href="#conclusion">
            Conclusion
          </a>
        </li>
      
    </ol>
  </nav>


    
    

<p>Where do we start when we stumble across a dataset we don’t know much about? Lets say one where we don’t necessarily understand the underlying generative process for some or all of the variables. Lets assume for now we’re sure there aren’t one off interventions or level shifts in the data, and we don’t know anything about the distribution of the features, trends, seasonality, model parameters, variance, etc.</p>

<p>I tend to start with the simplest, most interpretable models first, regardless if the problem requires classification, regression, or causality modeling. This allows me to assess how difficult the problem is before wasting time applying a complex solution.</p>

<p><img src="image0.gif" alt="gif" /></p>

<p>The IPython notebook below will outline exploratory analysis in terms of 1) Histograms and Aggregation, 2) Correlation Structure , 3) Dimensional Reduction. Note this isn’t meant to be an exhaustive effort to enumerate all types of imputation and pre-processing, but a quick examination of some best practices.</p>

<pre><code class="language-python">import time
import csv
import pickle
import random
import time
import numpy as np
import pandas as pd
import pylab as plt
import seaborn as sns
from functools import wraps
from matplotlib.font_manager import FontProperties
from scipy import interp
import matplotlib.pyplot as plt
from sklearn import cross_validation
from sklearn import neighbors
from sklearn import preprocessing
from sklearn import tree
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
from sklearn.learning_curve import learning_curve
from sklearn.metrics import accuracy_score
from sklearn.cross_validation import train_test_split
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA

%matplotlib inline

%%html
&lt;!-- just to make a markdown table pretty --&gt;
&lt;style&gt;table {float:left}&lt;/style&gt;
</code></pre>

<h2 id="our-example-data">Our Example Data</h2>

<p><a href="http://archive.ics.uci.edu/ml/datasets/Bank+Marketing">This data</a> is from phone call based direct marketing campaigns of a Portuguese banking institution. The output is whether a customer signed up for the financial product.</p>

<p>Why might this dataset be interesting?
There are progressively more platforms to reach customers such as social media, broadcast media, and physical advertisements. Due to the many input streams and the advancement of comparison shopping, conventional marketing campaigns have a reduced effect on the general public. Competition and budget constraints demand that marketing teams invest on highly refined and directed campaigns that maximize their Return On Investment. Modern campaigns can be created with the use of advanced analytics.</p>

<p>This is a particularly interesting dataset as it seeks to optimize an older and conventional business medium, where as recent optimization involves mobile applications or ad-sponsored streaming media. The data contains many interpretable social and economic features thus it will be interesting to see if the stratifying effects of these nominal, ordinal, and numeric variables is intuitive.</p>

<p>In the interest of time I won&rsquo;t step through our analysis and rationale of every single feature like I did in my Lending Club Data Analysis Blog Post, but we will explore a few. Our pre-processing includes dropping predictors with no power, dummy encoding, and standardization. This is a binary classification problem. What our data will end up like after pre-processing:
<table>
<thead>
<tr>
<th><strong>State</strong></th>
<th><strong>Observations</strong></th>
<th><strong>Features</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Original</strong></td>
<td>25211</td>
<td>20</td>
</tr>
<tr>
<td><strong>Pre-processing</strong></td>
<td>4119</td>
<td>63</td>
</tr>
</tbody>
</table></p>

<pre><code class="language-python">data = pd.read_csv('data/bank-additional1.csv')
# Data was already randomized by author
data_labels = pd.Series([0 if x == 'no' else 1 for x in data.ix[:,-1]])

print(&quot;Head of data&quot;)
print(data.ix[1])
</code></pre>

<pre><code>Head of data
age                        39
job                  services
marital                single
education         high.school
default                    no
housing                    no
loan                       no
contact             telephone
month                     may
day_of_week               fri
duration                  346
campaign                    4
pdays                     999
previous                    0
poutcome          nonexistent
emp.var.rate              1.1
cons.price.idx         93.994
cons.conf.idx           -36.4
euribor3m               4.855
nr.employed              5191
y                          no
Name: 1, dtype: object
</code></pre>

<h2 id="exploratory-analysis">Exploratory Analysis</h2>

<p>The typical things I do in order are as follows:</p>

<h3 id="1-histograms-and-aggregation-facets-pivots">1. Histograms and Aggregation/Facets/Pivots</h3>

<p>An easy place to start is to examine histograms (or scatterplots) of the features to discover characteristics such as sparsity, few unique values, distribution (is it heavy tailed or normally distributed).</p>

<p>Some plots are pretty intuitive like this facetgrid which shows density plots of the respondant age faceted by outcome label (yes/no) and if they were contacted by cell or land line. We can expect a larger count of older respondents for land lines.</p>

<pre><code class="language-python">g = sns.FacetGrid(data, row=&quot;contact&quot;, col=&quot;y&quot;, margin_titles=True)
g.map(sns.distplot, &quot;age&quot;)
</code></pre>

<p><img src="image1.png" alt="png" /></p>

<p>options=&ldquo;700x&rdquo; &gt;}}</p>

<p>Taking a look at the distribution of the ages in the dataset, it looks probably there wasn&rsquo;t a concerted effort to target a specific age class. The plot of the general workforce population might be similar.</p>

<pre><code class="language-python">data.age.hist(alpha=.6)
</code></pre>

<p><img src="image2.png" alt="png" /></p>

<h2 id="feature-target-distributions">Feature-Target Distributions</h2>

<p>These class histograms conditioned on feature values help us identify features with predictive power. Is there a telling difference between the age distribution? between respondents of different outcomes? Note that the histogram is expressed in densities and not in frequencies here for plotting purposes. We can tell very quickly that older respondents signed up for the solicited product more often in this sample. A box-plot could tell the same story.</p>

<pre><code class="language-python">sns.kdeplot(data.query(&quot;y == 'no'&quot;).age, shade=True, alpha=.2, label='No', color='salmon')
sns.kdeplot(data.query(&quot;y == 'yes'&quot;).age, shade=True, alpha=.2, label='Yes',color='dodgerblue')

plt.show()
</code></pre>

<p><img src="image3.png" alt="png" /></p>

<p>Let&rsquo;s examine our outcome variable pivoted by poutcome which describes the outcome of the previous marketing campaign/contact (&lsquo;failure&rsquo;,&lsquo;nonexistent&rsquo;,&lsquo;success&rsquo;).</p>

<pre><code class="language-python">gb = data.groupby(by=[&quot;poutcome&quot;,&quot;y&quot;])
gbs = gb.size()
gbs
</code></pre>

<pre><code>poutcome     y  
failure      no      387
             yes      67
nonexistent  no     3231
             yes     292
success      no       50
             yes      92
dtype: int64
</code></pre>

<p>Respondents that were a success before, were a success again at 10x the rate of previous failures.</p>

<pre><code class="language-python">gbs[1]/gbs[0] , gbs[3]/gbs[2], gbs[5]/gbs[4]
</code></pre>

<p><code>(0.1731266149870801, 0.090374497059733827, 1.8400000000000001)</code></p>

<h3 id="imbalanced-data">Imbalanced Data</h3>

<p>If we examine the table above we see a severe distortion of outcomes which may distort the performance of the algorithms we choose to deploy. Test accuracy is not robust in this situation as most models will tend to fit the majority class better (more observations) which may indirectly bring up the overall accuracy. If we care more about recall than precision, that is more success identifying respondents that will sign up for the banking product versus our general ability to classify yes vs no respondents, we might want to use the ROC (Receiver Operating Characteristic) curve and AUC (Area Under Curve ) as a a performance measurement. More on this in other general posts, but to drive intuition I&rsquo;ll give an example: In this dataset 90% of the responses are &lsquo;No&rsquo;. So if we created a naive model that always predicted &lsquo;No&rsquo;, we&rsquo;d have ridiculously high accuracy.</p>

<p>Another way to address this heavy biased data set is to create a new one with resampling such as sampling with replacement from the group with less data until the number equals to the larger group and vice versa.</p>

<p>UnbalancedDataset is a python package offering a number of re-sampling techniques commonly used in datasets showing strong between-class imbalance.</p>

<h3 id="missing-data">Missing Data</h3>

<p>Although not necessary to produce a plot for, tabular views of the data can help us discover variables that may need imputation. For a quick look at data imputation methods in Pandas look here. We can go from simple imputation like columnwise mean filling, or polynomial approximation for time series, to linear regression (using what we know about other variables), to complex meta-methods (ignoring our class and using ML algos to predict the missing data).</p>

<h2 id="correlation-structure">Correlation Structure</h2>

<p>Feature-Target Correlation
Here we can use correlation measures like mutual information, pearson&rsquo;s correlation coefficient, or chi-squared, based on the type of feature and class we are examining (categorical or numeric)</p>

<p>Below we use Mutual Information. There are many methods more advanced the typical mutual information measure we used here which uses Kullback-Leibler divergence. For example there are versions based on quadratic divergence measures which don&rsquo;t require prior assumptions about class densities (assumption of independence). In this example we see a similar importance for poutcome as we did before with our simple aggregation table.</p>

<pre><code class="language-python">from sklearn import metrics
f_list = ['education','job','marital','contact','campaign','duration','loan','poutcome']
sorted([(feature,metrics.adjusted_mutual_info_score(data[feature],data['y'])) for feature in f_list],
       key=lambda x: x[1], reverse=True)
</code></pre>

<pre><code>[('poutcome', 0.065587942670630867),
 ('contact', 0.015942038384609447),
 ('duration', 0.014895700838497688),
 ('job', 0.0031057910819885454),
 ('campaign', 0.0014596269197607321),
 ('education', 0.0011151723310124562),
 ('marital', 0.00089561182867500366),
 ('loan', -0.00018340180508122201)]
</code></pre>

<h3 id="how-many-features-do-we-have-really">How many features do we have really?</h3>

<p>Here we can use PCA or another dimensionality reduction tool to examine structure. Sometimes this allows us to quickly discover the true dimensionality of the data, do we really have 500 features or is it really only 50? If using PCA I like to take a look at the sorted eigenvalues and examine if their values tend to decay slowly or rapidly. If I notice a fast drop off I can identify that there is a lot of structure in fewer features. In the case where the drop off is slower, I might have a harder time finding structure in the data.</p>

<p>More concretely what we&rsquo;re doing is:</p>

<ul>
<li>Obtaining Eigenvectors and Eigenvalues from the covariance or correlation matrix.</li>
<li>Sorting eigenvalues in descending order to choose the k eigenvectors which correspond to the k largest eigenvalues where k is the number of dimensions of the new feature subspace (k≤d).</li>
<li>Construct the projection matrix W from the selected k eigenvectors.</li>
<li>Transform the original dataset X via W to obtain a k-dimensional feature subspace Y.</li>
</ul>

<p><em>Note: Eigendecomposition of the covariance or correlation matrix may be more intuitiuve; however, most PCA implementations perform a Singular Vector Decomposition (SVD) to improve the computational efficiency</em></p>

<p>For a simple intuition to PCA see an execllent tutorial <a href="http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html">here</a>.</p>

<p>Let&rsquo;s finish pre-processing our data to get a completely numerical representation of the feature space.</p>

<pre><code class="language-python"># Boiler Plate pre-processing

test_size=.33

pp_data = data.copy()

# Scale numeric data
dts_cols = [0,10, 11, 12,13,15,16,17,18,19]
data_to_scale = pp_data.iloc[:, dts_cols].astype(np.float)  # change int to float
scaler = preprocessing.StandardScaler().fit(data_to_scale)
pp_data.iloc[:, dts_cols] = scaler.transform(data_to_scale)

# Create dummy encoding for categorical data
dtde_cols = [1, 2, 3, 4, 5, 6, 7, 8, 9, 14]
data_to_de = pp_data.iloc[:,dtde_cols]
de_data = pd.get_dummies(data_to_de)
pp_data.drop(pp_data.columns[dtde_cols], axis=1, inplace=True)
pp_data = pp_data.merge(de_data, how='inner',left_index=True, right_index=True, copy=False)

pp_data.drop('y',1,inplace=True)

X = pp_data.values
y = data_labels.values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
</code></pre>

<p>Now back to where we were. We can see the features that contribute most to our two principal components. Interestingly enough nothing that we&rsquo;ve done so far hints to the warning the dataset provides about the strength of the duration variable.: <em>&ldquo;duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y=&lsquo;no&rsquo;). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.&rdquo;</em></p>

<p>If we stopped here with out examination we might not discover the flaw in using this feature until we got to our model training and tuning.</p>

<pre><code class="language-python">pca = PCA(n_components=2)
pca.fit_transform(pp_data)

pca_df = pd.DataFrame(pca.components_,columns=pp_data.columns,index = ['PC-1','PC-2'])

np.abs(pca_df.max(axis=0)).sort_values(ascending=False) # regardless of sign
</code></pre>

<pre><code>pdays                            0.598286
cons.price.idx                   0.325123
previous                         0.305417
emp.var.rate                     0.158449
euribor3m                        0.128620
poutcome_nonexistent             0.115631
contact_cellular                 0.110780
cons.conf.idx                    0.103740
contact_telephone                0.097095
poutcome_failure                 0.073725
marital_single                   0.049542
month_may                        0.043885
default_no                       0.040327
nr.employed                      0.038772
poutcome_success                 0.037754
month_apr                        0.033660
month_jun                        0.032950
duration                         0.026043
default_unknown                  0.021619
housing_yes                      0.021206
job_blue-collar                  0.021128
campaign                         0.020842
marital_married                  0.019902
month_aug                        0.019134
education_high.school            0.017522
housing_no                       0.016727
month_nov                        0.015502
education_basic.9y               0.014443
age                              0.013488
month_sep                        0.013096
                                   ...   
day_of_week_wed                  0.011663
month_jul                        0.008897
job_retired                      0.008825
month_mar                        0.008493
job_student                      0.008022
education_basic.4y               0.007861
day_of_week_thu                  0.006818
day_of_week_mon                  0.005996
loan_yes                         0.005809
job_admin.                       0.005244
month_dec                        0.004605
job_management                   0.003933
day_of_week_tue                  0.003701
education_basic.6y               0.003571
job_housemaid                    0.003172
marital_divorced                 0.002248
education_professional.course    0.002034
job_unemployed                   0.002003
education_unknown                0.001931
loan_no                          0.001364
job_entrepreneur                 0.001203
day_of_week_fri                  0.001194
job_unknown                      0.000768
loan_unknown                     0.000733
housing_unknown                  0.000733
education_illiterate             0.000159
job_self-employed                0.000154
marital_unknown                  0.000148
job_technician                   0.000055
default_yes                      0.000036
dtype: float64
</code></pre>

<pre><code class="language-python">np.abs(pca_df.max(axis=0)).sort_values(ascending=False).plot()
plt.xticks(rotation='vertical')
</code></pre>

<p><img src="image4.png" alt="png" /></p>

<h3 id="numeric-correlation">Numeric Correlation</h3>

<p>We have a lot of dummy encoded variables so I&rsquo;m expecting the outcome of a straight numerical correlation won&rsquo;t be too telling. We usually examine our dataframes correlation matrix and drop highly correlated/redundant data to address multicollinearity.</p>

<pre><code class="language-python">cor = pp_data.corr()
cor.loc[:,:] = np.tril(cor, k=-1) # below main lower triangle of an array
cor = cor.stack()
(cor[(cor &gt; 0.55) | (cor &lt; -0.55)]).sort_values()
</code></pre>

<pre><code>contact_telephone     contact_cellular   -1.000000
default_unknown       default_no         -0.999227
housing_yes           housing_no         -0.949959
poutcome_success      pdays              -0.940565
loan_yes              loan_no            -0.915086
poutcome_nonexistent  poutcome_failure   -0.855705
                      previous           -0.854241
marital_single        marital_married    -0.778335
previous              pdays              -0.587941
contact_cellular      cons.price.idx     -0.574452
contact_telephone     cons.price.idx      0.574452
euribor3m             cons.price.idx      0.657159
poutcome_failure      previous            0.661990
cons.price.idx        emp.var.rate        0.755155
nr.employed           emp.var.rate        0.897173
                      euribor3m           0.942589
euribor3m             emp.var.rate        0.970308
loan_unknown          housing_unknown     1.000000
dtype: float64
</code></pre>

<h2 id="dimensional-reduction">Dimensional Reduction</h2>

<p>If low dimensional structure, there are off the shelf sets of non-linear dimensionality reductions tools such as T-SNE, Isomap, LOE, variants of PCA or variants of multidimensional scaling with some interesting additional non-linearity. Sometimes these can help us find out manifold or cluster structure and can be helpful for identifying important variations.</p>

<pre><code class="language-python">from sklearn.manifold import TSNE

t_X = pp_data.ix[:,:10].values # lets only use the standard scaled data

# perform t-SNE embedding
tsne = TSNE(n_components=2, init='random', random_state=0)
Y = tsne.fit_transform(t_X)

plt.scatter(Y[:, 0], Y[:, 1], c=y, cmap=plt.cm.Set3,s=30,alpha=.8)
# plt.axis('tight')
</code></pre>

<p><img src="image5.png" alt="png" /></p>

<p>I haven&rsquo;t used T-SNE in any meaningful way for years, but some key things to note are if we get a representation where regions overlap less compared to a standard post 2-dimension PCA plot. This might suggest that this dataset can be separated more effectively by non-linear methods that may focus on the local structure. We might find that 2-dimensions isn&rsquo;t enough to accurately represent the internal structure of the data (we might want to look at some 3-D plots).</p>

<h2 id="conclusion">Conclusion</h2>

<p>Thanks for reading! I might have spent more time on making the header image in photoshop than on the code :)</p>

<p>Let me know via email or twitter if you have any sections you&rsquo;d like me to expand on. Hope to see you at <a href="https://us.pycon.org/2016/">PyCon 2016 in Oregon</a>!</p>

  </main>

          <footer role="contentinfo">
  <div>
    <label for="themer">
      dark theme: <input type="checkbox" id="themer" class="vh">
      <span aria-hidden="true"></span>
    </label>
  </div>
  
    Made with <a href="https://gohugo.io/">Hugo</a>. Deployed to <a href="https://app.netlify.com/">Netlify</a>. Source on <a href="https://github.com/kevindavenport/kldavenport.com">Github</a>.
  
</footer>

        </div>
      </div>
    </div>
    <script src="https://kldavenport.netlify.com/js/prism.js"></script>
<script src="https://kldavenport.netlify.com/js/dom-scripts.js"></script>

    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-34706513-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>
