<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.54.0" />
  <link rel="canonical" href="https://kldavenport.netlify.com/lending-club-python/">

  

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://kldavenport.netlify.com/css/prism.css" media="none" onload="this.media='all';">

  
  
  <link rel="stylesheet" type="text/css" href="https://kldavenport.netlify.com/css/styles.css">

  <style id="inverter" media="none">
    html { filter: invert(100%) }
    * { background-color: inherit }
    img:not([src*=".svg"]), .colors, iframe, .demo-container { filter: invert(100%) }
  </style>

  
  
  <title>Lending Club Data Analysis Revisited with Python | Kevin Davenport Engineering &amp; ML blog</title>
</head>

  <body>
    <a href="#main">skip to content</a>
    <svg style="display: none">
  <symbol id="bookmark" viewBox="0 0 40 50">
   <g transform="translate(2266 3206.2)">
    <path style="stroke:currentColor;stroke-width:3.2637;fill:none" d="m-2262.2-3203.4-.2331 42.195 16.319-16.318 16.318 16.318.2331-42.428z"/>
   </g>
  </symbol>

  <symbol id="w3c" viewBox="0 0 127.09899 67.763">
   <text font-size="83" style="font-size:83px;font-family:Trebuchet;letter-spacing:-12;fill-opacity:0" letter-spacing="-12" y="67.609352" x="-26.782778">W3C</text>
   <text font-size="83" style="font-size:83px;font-weight:bold;font-family:Trebuchet;fill-opacity:0" y="67.609352" x="153.21722" font-weight="bold">SVG</text>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m33.695.377 12.062 41.016 12.067-41.016h8.731l-19.968 67.386h-.831l-12.48-41.759-12.479 41.759h-.832l-19.965-67.386h8.736l12.061 41.016 8.154-27.618-3.993-13.397h8.737z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m91.355 46.132c0 6.104-1.624 11.234-4.862 15.394-3.248 4.158-7.45 6.237-12.607 6.237-3.882 0-7.263-1.238-10.148-3.702-2.885-2.47-5.02-5.812-6.406-10.022l6.82-2.829c1.001 2.552 2.317 4.562 3.953 6.028 1.636 1.469 3.56 2.207 5.781 2.207 2.329 0 4.3-1.306 5.909-3.911 1.609-2.606 2.411-5.738 2.411-9.401 0-4.049-.861-7.179-2.582-9.399-1.995-2.604-5.129-3.912-9.397-3.912h-3.327v-3.991l11.646-20.133h-14.062l-3.911 6.655h-2.493v-14.976h32.441v4.075l-12.31 21.217c4.324 1.385 7.596 3.911 9.815 7.571 2.22 3.659 3.329 7.953 3.329 12.892z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.21 0 1.414 8.6-5.008 9.583s-1.924-4.064-5.117-6.314c-2.693-1.899-4.447-2.309-7.186-1.746-3.527.73-7.516 4.938-9.258 10.13-2.084 6.21-2.104 9.218-2.178 11.978-.115 4.428.58 7.043.58 7.043s-3.04-5.626-3.011-13.866c.018-5.882.947-11.218 3.666-16.479 2.404-4.627 5.954-7.404 9.114-7.728 3.264-.343 5.848 1.229 7.841 2.938 2.089 1.788 4.213 5.698 4.213 5.698l4.94-9.837z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.82 48.674s-2.208 3.957-3.589 5.48c-1.379 1.524-3.849 4.209-6.896 5.555-3.049 1.343-4.646 1.598-7.661 1.306-3.01-.29-5.807-2.032-6.786-2.764-.979-.722-3.486-2.864-4.897-4.854-1.42-2-3.634-5.995-3.634-5.995s1.233 4.001 2.007 5.699c.442.977 1.81 3.965 3.749 6.572 1.805 2.425 5.315 6.604 10.652 7.545 5.336.945 9.002-1.449 9.907-2.031.907-.578 2.819-2.178 4.032-3.475 1.264-1.351 2.459-3.079 3.116-4.108.487-.758 1.276-2.286 1.276-2.286l-1.276-6.644z"/>
  </symbol>

  <symbol id="tag" viewBox="0 0 177.16535 177.16535">
    <g transform="translate(0 -875.2)">
     <path style="fill-rule:evenodd;stroke-width:0;fill:currentColor" d="m159.9 894.3-68.79 8.5872-75.42 77.336 61.931 60.397 75.429-76.565 6.8495-69.755zm-31.412 31.835a10.813 10.813 0 0 1 1.8443 2.247 10.813 10.813 0 0 1 -3.5174 14.872l-.0445.0275a10.813 10.813 0 0 1 -14.86 -3.5714 10.813 10.813 0 0 1 3.5563 -14.863 10.813 10.813 0 0 1 13.022 1.2884z"/>
    </g>
  </symbol>

  <symbol id="balloon" viewBox="0 0 141.73228 177.16535">
   <g transform="translate(0 -875.2)">
    <g>
     <path style="fill:currentColor" d="m68.156 882.83-.88753 1.4269c-4.9564 7.9666-6.3764 17.321-5.6731 37.378.36584 10.437 1.1246 23.51 1.6874 29.062.38895 3.8372 3.8278 32.454 4.6105 38.459 4.6694-.24176 9.2946.2879 14.377 1.481 1.2359-3.2937 5.2496-13.088 8.886-21.623 6.249-14.668 8.4128-21.264 10.253-31.252 1.2464-6.7626 1.6341-12.156 1.4204-19.764-.36325-12.93-2.1234-19.487-6.9377-25.843-2.0833-2.7507-6.9865-7.6112-7.9127-7.8436-.79716-.20019-6.6946-1.0922-6.7755-1.0248-.02213.0182-5.0006-.41858-7.5248-.22808l-2.149-.22808h-3.3738z"/>
     <path style="fill:currentColor" d="m61.915 883.28-3.2484.4497c-1.7863.24724-3.5182.53481-3.8494.63994-2.4751.33811-4.7267.86957-6.7777 1.5696-.28598 0-1.0254.20146-2.3695.58589-5.0418 1.4418-6.6374 2.2604-8.2567 4.2364-6.281 7.6657-11.457 18.43-12.932 26.891-1.4667 8.4111.71353 22.583 5.0764 32.996 3.8064 9.0852 13.569 25.149 22.801 37.517 1.3741 1.841 2.1708 2.9286 2.4712 3.5792 3.5437-1.1699 6.8496-1.9336 10.082-2.3263-1.3569-5.7831-4.6968-21.86-6.8361-33.002-.92884-4.8368-2.4692-14.322-3.2452-19.991-.68557-5.0083-.77707-6.9534-.74159-15.791.04316-10.803.41822-16.162 1.5026-21.503 1.4593-5.9026 3.3494-11.077 6.3247-15.852z"/>
     <path style="fill:currentColor" d="m94.499 885.78c-.10214-.0109-.13691 0-.0907.0409.16033.13489 1.329 1.0675 2.5976 2.0723 6.7003 5.307 11.273 14.568 12.658 25.638.52519 4.1949.24765 14.361-.5059 18.523-2.4775 13.684-9.7807 32.345-20.944 53.519l-3.0559 5.7971c2.8082.76579 5.7915 1.727 8.9926 2.8441 11.562-11.691 18.349-19.678 24.129-28.394 7.8992-11.913 11.132-20.234 12.24-31.518.98442-10.02-1.5579-20.876-6.7799-28.959-.2758-.4269-.57803-.86856-.89617-1.3166-3.247-6.13-9.752-12.053-21.264-16.131-2.3687-.86369-6.3657-2.0433-7.0802-2.1166z"/>
     <path style="fill:currentColor" d="m32.52 892.22c-.20090-.13016-1.4606.81389-3.9132 2.7457-11.486 9.0476-17.632 24.186-16.078 39.61.79699 7.9138 2.4066 13.505 5.9184 20.562 5.8577 11.77 14.749 23.219 30.087 38.74.05838.059.12188.1244.18052.1838 1.3166-.5556 2.5965-1.0618 3.8429-1.5199-.66408-.32448-1.4608-1.3297-3.8116-4.4602-5.0951-6.785-8.7512-11.962-13.051-18.486-5.1379-7.7948-5.0097-7.5894-8.0586-13.054-6.2097-11.13-8.2674-17.725-8.6014-27.563-.21552-6.3494.13041-9.2733 1.775-14.987 2.1832-7.5849 3.9273-10.986 9.2693-18.07 1.7839-2.3656 2.6418-3.57 2.4409-3.7003z"/>
     <path style="fill:currentColor" d="m69.133 992.37c-6.2405.0309-12.635.76718-19.554 2.5706 4.6956 4.7759 9.935 10.258 12.05 12.625l4.1272 4.6202h11.493l3.964-4.4516c2.0962-2.3541 7.4804-7.9845 12.201-12.768-8.378-1.4975-16.207-2.6353-24.281-2.5955z"/>
     <rect style="stroke-width:0;fill:currentColor" ry="2.0328" height="27.746" width="22.766" y="1017.7" x="60.201"/>
    </g>
   </g>
  </symbol>

  <symbol id="info" viewBox="0 0 41.667 41.667">
   <g transform="translate(-37.035 -1004.6)">
    <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m76.25 1030.2a18.968 18.968 0 0 1 -23.037 13.709 18.968 18.968 0 0 1 -13.738 -23.019 18.968 18.968 0 0 1 23.001 -13.768 18.968 18.968 0 0 1 13.798 22.984"/>
    <g transform="matrix(1.1146 0 0 1.1146 -26.276 -124.92)">
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m75.491 1039.5v-8.7472"/>
     <path style="stroke-width:0;fill:currentColor" transform="scale(-1)" d="m-73.193-1024.5a2.3719 2.3719 0 0 1 -2.8807 1.7142 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
   </g>
  </symbol>

  <symbol id="warning" viewBox="0 0 48.430474 41.646302">
    <g transform="translate(-1.1273 -1010.2)">
     <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:4.151;fill:none" d="m25.343 1012.3-22.14 37.496h44.28z"/>
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:4.1512;fill:none" d="m25.54 1027.7v8.7472"/>
     <path style="stroke-width:0;fill:currentColor" d="m27.839 1042.8a2.3719 2.3719 0 0 1 -2.8807 1.7143 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
  </symbol>

  <symbol id="menu" viewBox="0 0 50 50">
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="0" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="20" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="40" x="0"/>
   </symbol>

   <symbol id="link" viewBox="0 0 50 50">
    <g transform="translate(0 -1002.4)">
     <g transform="matrix(.095670 0 0 .095670 2.3233 1004.9)">
      <g>
       <path style="stroke-width:0;fill:currentColor" d="m452.84 192.9-128.65 128.65c-35.535 35.54-93.108 35.54-128.65 0l-42.881-42.886 42.881-42.876 42.884 42.876c11.845 11.822 31.064 11.846 42.886 0l128.64-128.64c11.816-11.831 11.816-31.066 0-42.9l-42.881-42.881c-11.822-11.814-31.064-11.814-42.887 0l-45.928 45.936c-21.292-12.531-45.491-17.905-69.449-16.291l72.501-72.526c35.535-35.521 93.136-35.521 128.64 0l42.886 42.881c35.535 35.523 35.535 93.141-.001 128.66zm-254.28 168.51-45.903 45.9c-11.845 11.846-31.064 11.817-42.881 0l-42.884-42.881c-11.845-11.821-11.845-31.041 0-42.886l128.65-128.65c11.819-11.814 31.069-11.814 42.884 0l42.886 42.886 42.876-42.886-42.876-42.881c-35.54-35.521-93.113-35.521-128.65 0l-128.65 128.64c-35.538 35.545-35.538 93.146 0 128.65l42.883 42.882c35.51 35.54 93.11 35.54 128.65 0l72.496-72.499c-23.956 1.597-48.092-3.784-69.474-16.283z"/>
      </g>
     </g>
    </g>
  </symbol>

  <symbol id="doc" viewBox="0 0 35 45">
   <g transform="translate(-147.53 -539.83)">
    <path style="stroke:currentColor;stroke-width:2.4501;fill:none" d="m149.38 542.67v39.194h31.354v-39.194z"/>
    <g style="stroke-width:25" transform="matrix(.098003 0 0 .098003 133.69 525.96)">
     <path d="m220 252.36h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path style="stroke:currentColor;stroke-width:25;fill:none" d="m220 409.95h200"/>
     <path d="m220 488.74h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path d="m220 331.15h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
    </g>
   </g>
 </symbol>

 <symbol id="tick" viewBox="0 0 177.16535 177.16535">
  <g transform="translate(0 -875.2)">
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="155" width="40" y="702.99" x="556.82"/>
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="40" width="90.404" y="817.99" x="506.42"/>
  </g>
 </symbol>
</svg>

    <div class="wrapper">
      <header class="intro-and-nav" role="banner">
  <div>
    <div class="intro">
      <a class="logo" href="/" aria-label="Kevin Davenport Engineering &amp; ML blog home page">
        <img src="https://kldavenport.netlify.com/images/logo.svg" alt="">
      </a>
      <p class="library-desc">
        
      </p>
    </div>
    <nav id="patterns-nav" class="patterns" role="navigation">
  <h2 class="vh">Main navigation</h2>
  <button id="menu-button" aria-expanded="false">
    <svg viewBox="0 0 50 50" aria-hidden="true" focusable="false">
      <use xlink:href="#menu"></use>
    </svg>
    Menu
  </button>
  
  <ul id="patterns-list">
  
    <li class="pattern">
      
      
      
      
      <a href="/post/" aria-current="page">
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Blog</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/about/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">About</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/bike-travel/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Bike Travel</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/tags/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Tags</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/index.xml" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">RSS</span>
      </a>
    </li>
  
  </ul>
</nav>
  </div>
</header>
      <div class="main-and-footer">
        <div>
          
  <main id="main">
    <h1>
      <svg class="bookmark-icon" aria-hidden="true" viewBox="0 0 40 50" focusable="false">
        <use xlink:href="#bookmark"></use>
      </svg>
      Lending Club Data Analysis Revisited with Python
    </h1>
    <div class="date">
      
      <strong aria-hidden="true">Publish date: </strong>Oct 17, 2015
    </div>
    
      <div class="tags">
        <strong aria-hidden="true">Tags: </strong>
        <ul aria-label="tags">
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/finance">finance</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/gradient-boosting">gradient boosting</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/decision-trees">decision trees</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/data-science">data science</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/data-exploration">data exploration</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/data-engineering">data engineering</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/correlation">correlation</a>
            </li>
          
        </ul>
      </div>
    
    
    
      

  <nav class="toc" aria-labelledby="toc-heading">
    <h2 id="toc-heading">Table of contents</h2>
    <ol>
      
        <li>
          <a href="#our-data">
            Our Data
          </a>
        </li>
      
        <li>
          <a href="#keeping-what-we-need">
            Keeping what we need
          </a>
        </li>
      
        <li>
          <a href="#loan-amount-requested-verus-the-funded-amount">
            Loan Amount Requested Verus the Funded Amount
          </a>
        </li>
      
        <li>
          <a href="#employment-title">
            Employment Title
          </a>
        </li>
      
        <li>
          <a href="#employment-length">
            Employment Length
          </a>
        </li>
      
        <li>
          <a href="#loan-statuses">
            Loan Statuses
          </a>
        </li>
      
        <li>
          <a href="#arbitrary-applicant-input">
            Arbitrary Applicant Input
          </a>
        </li>
      
        <li>
          <a href="#delinquency">
            Delinquency
          </a>
        </li>
      
        <li>
          <a href="#revolving-utility">
            Revolving Utility
          </a>
        </li>
      
        <li>
          <a href="#more-post-loan-attributes">
            More Post Loan Attributes
          </a>
        </li>
      
        <li>
          <a href="#last-fico-ranges">
            Last FICO Ranges
          </a>
        </li>
      
        <li>
          <a href="#wrapping-up-the-data-munging">
            Wrapping Up the Data Munging
          </a>
        </li>
      
        <li>
          <a href="#our-model">
            Our model
          </a>
        </li>
      
        <li>
          <a href="#gradient-boosted-regression-trees">
            Gradient Boosted Regression Trees
          </a>
        </li>
      
        <li>
          <a href="#training-tuning">
            Training &amp; Tuning
          </a>
        </li>
      
        <li>
          <a href="#examing-results">
            Examing results
          </a>
        </li>
      
        <li>
          <a href="#partial-dependence-plots">
            Partial Dependence Plots
          </a>
        </li>
      
    </ol>
  </nav>


    
    

<p>2.5 years ago I analyzed Lending Club’s issued loans data (I was using R back then!). It was the most visited blog post on my site in 2013 through 2014. Today it’s still number 5. Reddit picked up my simple “35-hour work week with Python” post which is now #1.</p>

<p><a href="https://www.lendingclub.com">Lending Club</a> is the first peer-to-peer lending company to register its offerings as securities with the Securities and Exchange Commission (SEC). Their operational statistics are public and available for download. It has been a while since I’ve posted an end to end solution blog post and would like to replicate the post with a bit more sophistication in Python with the latest dataset from lendinglub.com. In summary, let’s examine all the attributes Lending Club collects on users and how they influence the interest rates issued.</p>

<p><img src="image0.png" alt="png" /></p>

<p>We&rsquo;ll download the 2013-2014 data and uncompress it from inside our notebook by invoking the command line (I didn&rsquo;t feel like installing wget on OSX but at least we have curl:</p>

<pre><code class="language-bash">%%bash
curl https://resources.lendingclub.com/LoanStats3c_securev.csv.zip | tar -xf-
</code></pre>

<p>Let&rsquo;s also take a quick look at the data via bash (file size, head, line count, column count).</p>

<pre><code class="language-bash">!du -h LoanStats3c_securev.csv
!head -n 2 LoanStats3c_securev.csv
</code></pre>

<p>Examining the data we see that most of feature names are intuitive. We can get the specifics from the provided data dictionary at <a href="https://www.lendingclub.com/info/download-data.action">https://www.lendingclub.com/info/download-data.action</a></p>

<pre><code class="language-bash">!wc -l &lt; LoanStats3c_securev.csv
!head -2 LoanStats3c_securev.csv | sed 's/[^,]//g' | wc -c
</code></pre>

<p>It looks we have 235,630 entries and 57 attributes. At a glance some of the more notable atributes are:</p>

<ul>
<li>Amount Requested</li>
<li>Amount Funded by Investors*</li>
<li>Interest Rate</li>
<li>Term (Loan Length)</li>
<li>Purpose of Loan</li>
<li>Debt/Income Ratio **</li>
<li>State</li>
<li>Rent or Own Home</li>
<li>Annual Income</li>
<li>30+ days past-due incidences of delinquency in the borrower&rsquo;s credit file for the past 2 years</li>
<li>FICO Low</li>
<li>FICO High</li>
<li>Last FICO Low</li>
<li>Last FiCO High</li>
<li>Credit Lines Open</li>
<li>Revolving Balance</li>
<li>Inquiries in Last 6 Months</li>
<li>Length of Employment</li>
</ul>

<p>The last vs current FICO scores did not exist the last time we looked at the data, I&rsquo;m thinking they added this to asses directionality or momentum, although this few of samples is suspicious. Interestingly enough when looking through the attributes provided for declined loans I found a &ldquo;score&rdquo; attribute which was desribed as such:
<em>&ldquo;For applications prior to November 5, 2013 the risk score is the borrower&rsquo;s FICO score. For applications after November 5, 2013 the risk score is the borrower&rsquo;s Vantage score.&rdquo;</em></p>

<p>I suspect they are actually using both.</p>

<p>* Lending club states that the amount funded by investors has no affect on the final interest rate assigned to a loan.</p>

<p>** Debt/Income Ratio ratio takes all of your monthly liabilities and divides the total by your gross monthly income.</p>

<h2 id="our-data">Our Data</h2>

<p>Let&rsquo;s use Pandas to manipulate our data and find an excuse to use the new pipe operators <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pipe.html">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pipe.html</a></p>

<p>Our data is 126mb on disk, 102.2 in memory, not the greatest reduction in side I&rsquo;ve seen, but we have a lot of inefficient object data types in the df to start.</p>

<pre><code class="language-python">mport numpy as np
import pandas as pd
import seaborn as sns
%matplotlib inline

df = pd.read_csv(&quot;LoanStats3c_securev.csv&quot;,skiprows=1)
df.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 235629 entries, 0 to 235628
Data columns (total 56 columns):
id                             235629 non-null int64
member_id                      235629 non-null int64
loan_amnt                      235629 non-null int64
funded_amnt                    235629 non-null int64
funded_amnt_inv                235629 non-null int64
term                           235629 non-null object
int_rate                       235629 non-null object
installment                    235629 non-null float64
grade                          235629 non-null object
sub_grade                      235629 non-null object
emp_title                      222393 non-null object
emp_length                     235629 non-null object
home_ownership                 235629 non-null object
annual_inc                     235629 non-null float64
verification_status            235629 non-null object
issue_d                        235629 non-null object
loan_status                    235629 non-null object
pymnt_plan                     235629 non-null object
url                            235629 non-null object
desc                           15279 non-null object
purpose                        235629 non-null object
title                          235629 non-null object
zip_code                       235629 non-null object
addr_state                     235629 non-null object
dti                            235629 non-null float64
delinq_2yrs                    235629 non-null int64
earliest_cr_line               235629 non-null object
fico_range_low                 235629 non-null int64
fico_range_high                235629 non-null int64
inq_last_6mths                 235629 non-null int64
mths_since_last_delinq         119748 non-null float64
mths_since_last_record         41524 non-null float64
open_acc                       235629 non-null int64
pub_rec                        235629 non-null int64
revol_bal                      235629 non-null int64
revol_util                     235504 non-null object
total_acc                      235629 non-null int64
initial_list_status            235629 non-null object
out_prncp                      235629 non-null float64
out_prncp_inv                  235629 non-null float64
total_pymnt                    235629 non-null float64
total_pymnt_inv                235629 non-null float64
total_rec_prncp                235629 non-null float64
total_rec_int                  235629 non-null float64
total_rec_late_fee             235629 non-null float64
recoveries                     235629 non-null float64
collection_recovery_fee        235629 non-null float64
last_pymnt_d                   235486 non-null object
last_pymnt_amnt                235629 non-null float64
next_pymnt_d                   200795 non-null object
last_credit_pull_d             235602 non-null object
last_fico_range_high           235629 non-null int64
last_fico_range_low            235629 non-null int64
collections_12_mths_ex_med     235629 non-null int64
mths_since_last_major_derog    66478 non-null float64
policy_code                    235629 non-null int64
dtypes: float64(16), int64(17), object(23)
memory usage: 102.5+ MB
</code></pre>

<pre><code class="language-python">df.head(3)
</code></pre>

<p><img src="image1.png" alt="png" /></p>

<h2 id="keeping-what-we-need">Keeping what we need</h2>

<p>Let&rsquo;s work through our list of attributes slice-by-slice and figure out what we really need. This involves dropping some useless attributes and cleaning-up/modifying others. Let&rsquo;s work through the columns in batches to keep the cognitive burden low:</p>

<pre><code class="language-python"># .ix[row slice, column slice] 
df.ix[:4,:7]
</code></pre>

<table border="1">
  <thead>
    <tr>
      <th></th>
      <th>id</th>
      <th>member_id</th>
      <th>loan_amnt</th>
      <th>funded_amnt</th>
      <th>funded_amnt_inv</th>
      <th>term</th>
      <th>int_rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>36805548</td>
      <td>39558264</td>
      <td>10400</td>
      <td>10400</td>
      <td>10400</td>
      <td>36 months</td>
      <td>6.99%</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38098114</td>
      <td>40860827</td>
      <td>15000</td>
      <td>15000</td>
      <td>15000</td>
      <td>60 months</td>
      <td>12.39%</td>
    </tr>
    <tr>
      <th>2</th>
      <td>37662224</td>
      <td>40425321</td>
      <td>7650</td>
      <td>7650</td>
      <td>7650</td>
      <td>36 months</td>
      <td>13.66%</td>
    </tr>
    <tr>
      <th>3</th>
      <td>37612354</td>
      <td>40375473</td>
      <td>12800</td>
      <td>12800</td>
      <td>12800</td>
      <td>60 months</td>
      <td>17.14%</td>
    </tr>
    <tr>
      <th>4</th>
      <td>37822187</td>
      <td>40585251</td>
      <td>9600</td>
      <td>9600</td>
      <td>9600</td>
      <td>36 months</td>
      <td>13.66%</td>
    </tr>
  </tbody>
</table>

<ol>
<li>We won&rsquo;t need id or member_id as it has no real predictive power so we can drop them from this table</li>
<li>int_rate was loaded as an object data type instead of float due to the &lsquo;%&rsquo; character. Let&rsquo;s strip that out and convert the column type.</li>
</ol>

<pre><code class="language-python">df.drop(['id','member_id'],1, inplace=True)
df.int_rate = pd.Series(df.int_rate).str.replace('%', '').astype(float)
</code></pre>

<h2 id="loan-amount-requested-verus-the-funded-amount">Loan Amount Requested Verus the Funded Amount</h2>

<p>At a glance I thought it important to discover the relationship between loan amount requested verus the funded amount to see if Lending Club ever issues a lower amount than what is asked and <strong>why</strong>. However after taking a look at the data dictionary I discovered that the data has already been manipulated:</p>

<p><em>&ldquo;The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.&rdquo;</em></p>

<p>Alas, it would have been interesting to see how often Lending Club issues a loan that differs from the requested amount. Doing a quick santiy check below we see there are no instances in the data where the reuqested amount doesn&rsquo;t match the funded.</p>

<pre><code class="language-python">print (df.loan_amnt != df.funded_amnt).value_counts() # Instead of old subset syntax let's use the new `query` method.

# df.query('loan_amnt != funded_amnt').ix[:,:5].head(3) # Old way
</code></pre>

<p><code>False    235629
dtype: int64</code></p>

<p>Let&rsquo;s keep moving through the columns</p>

<pre><code class="language-python">df.ix[:5,8:15]
</code></pre>

<p><img src="image2.png" alt="png" /></p>

<h2 id="employment-title">Employment Title</h2>

<p>emp_title might be a free text field on the application form or a list of currated employment titles. Let&rsquo;s examine how many unique values exist:</p>

<pre><code class="language-python">print df.emp_title.value_counts().head()
print df.emp_title.value_counts().tail()
df.emp_title.unique().shape
</code></pre>

<pre><code>Teacher             4569
Manager             3772
Registered Nurse    1960
RN                  1816
Supervisor          1663
Name: emp_title, dtype: int64
Teacher/Media Specialist          1
teacher/ After School Director    1
bpd/gms coordinator               1
heavy equip operator              1
Corp. Operations Mangager         1
Name: emp_title, dtype: int64
</code></pre>

<p>75,353 unique entries among 235,630 observations seems like a bit of a stretch. Taking a look at the head vs tail of the doc shows some suspiciously specific titles such as bpd/gms coordinator. I feel comfortable assessing that this data won&rsquo;t be meaningful and any relationship we might observe might be due to confounding relationships. A more advanced implementation might look to group all these job descriptions into categories and/or examine if Lending Club&rsquo;s model looks at (income + job) versus just income, but that&rsquo;s out of the scope of this post.</p>

<p>Here we define a confounding variable as a variable that obscures the effects of another variable. If one 1st grade teacher used a phonics textbook in her class and a different teacher used a whole language textbook in his class, and students in the two classes were given achievement tests to see how well they read, the independent variables (teacher effectiveness and textbooks) would be confounded. There is no way to determine if differences in reading between the two classes were caused by either or both of the independent variables.</p>

<p>Applying this example to our dataset, Registered Nurses, or RNs, who have higher education requirements and receive above average pay, might be granted A grade loans on average. Is this due to them working as RNs or having a 4-year degree or their salary? Would we see the same effect for Physician Assistants who go to school for longer and receive even more pay? What about Certified Nursing Assistants (CNAs) who are on the oppposite spectrum?</p>

<pre><code class="language-python">df.drop(['emp_title'],1, inplace=True)
</code></pre>

<h2 id="employment-length">Employment Length</h2>

<p>Leaving this variable in might contradict our decision to drop the employment tile as it also conveyed a sort of socio-economic seniority. A Computer Scientist 5 years into their career would generally have a larger salary than a Kindergarden teacher 10 years into their career. Arguably it might be powerful to combine a grouped, matched, and reduced set of employment titles with their length to create a &ldquo;purchasing power&rdquo; metric. Since employment length is an easy scalar, let&rsquo;s leave it in for now.</p>

<p>We could leave &lsquo;emp_length&rsquo; as categorical data, but it shouldn&rsquo;t be treated as such or as ordinal data since the intervals are easy to determine. We can convert it into numerical data with a simple filter:</p>

<pre><code class="language-python">df.emp_length.value_counts()
</code></pre>

<pre><code>10+ years    79505
2 years      20487
3 years      18267
&lt; 1 year     17982
1 year       14593
4 years      13528
7 years      13099
5 years      13051
n/a          12019
8 years      11853
6 years      11821
9 years       9424
</code></pre>

<pre><code class="language-python">df.replace('n/a', np.nan,inplace=True)
df.emp_length.fillna(value=0,inplace=True)
df['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)
df['emp_length'] = df['emp_length'].astype(int)
</code></pre>

<p>Verification Status
The values of <code>verification_status</code> aren&rsquo;t immediately clear. As <code>VERIFIED - income and VERIFIED - income source</code> could potentially be the same criteria. The provided data dictionary file describes the field as such: <em>&ldquo;Indicates if income was verified by LC, not verified, or if the income source was verified&rdquo;</em> This description does not help as you&rsquo;d intuitively think that you would have to verify the source in order to verify income. For now we&rsquo;ll trust that these are indeed different methods or levels (perhaps even ordinal) of verification.</p>

<pre><code class="language-python">df.verification_status.value_counts()
</code></pre>

<pre><code>VERIFIED - income source    97740
not verified                70661
VERIFIED - income           67228
Name: verification_status, dtype: int64
</code></pre>

<h2 id="loan-statuses">Loan Statuses</h2>

<p>Moving on to the last column in the slice of the df we are examining: loan status is mutable value that represents the current state of the loan. If anything we might want to examine if all the independent variables and/or interst rate to determine the probability of the loan status at some time. In this post however we are focused on predicting the interest rate granted to an applicant at loan creation. Thus we do not care about this variable and will drop it after examining it out of curiosity.</p>

<p>When examining the distribution among the loan statuses, October has the highest amount of loans right ahead of the holiday season. To clean up the space let&rsquo;s replace the column value with a month only and convert the column back to a string or object type.</p>

<pre><code class="language-python">print df.loan_status.value_counts()

issue_d_todate = pd.to_datetime(df.issue_d)# (df['issue_d'].apply(lambda x: x.strftime('%Y-%m-%d')))
df.issue_d = pd.Series(df.issue_d).str.replace('-2014', '')
# We need sort_index() or else we won't get a sequential timedate order.
issue_d_todate.value_counts().sort_index().plot(kind='bar')

df.drop(['loan_status'],1, inplace=True)
</code></pre>

<pre><code>
Current               194161
Fully Paid             29346
Charged Off             5488
Late (31-120 days)      3738
In Grace Period         1996
Late (16-30 days)        824
Default                   76
</code></pre>

<p><img src="image3.png" alt="png" /></p>

<h2 id="arbitrary-applicant-input">Arbitrary Applicant Input</h2>

<p>Continuing on to examine more columns:</p>

<pre><code class="language-python">df.ix[:5,12:21]
</code></pre>

<p><img src="image4.png" alt="png" /></p>

<pre><code class="language-python">print df.purpose.value_counts()
print ''
print df.title.value_counts().tail()
</code></pre>

<pre><code>debt_consolidation    143006
credit_card            55522
home_improvement       13045
other                  10371
major_purchase          3858
medical                 2331
small_business          2277
car                     1832
moving                  1328
vacation                1178
house                    750
renewable_energy         123
wedding                    8
Name: purpose, dtype: int64

consolodate                       1
Chase debt                        1
pay off credit cards and loans    1
Business Consolidation Loan       1
Bills Bills Bills                 1
</code></pre>

<p>We&rsquo;ll be dropping <code>pymnt_plan</code> as it has the same current state info that <code>loan_status</code> had above. It indicates that the loan is in jeopardy and that the borrower has been placed on a payment plan.</p>

<p>Examining the unique counts of purpose show that they are lending club selected options;however, <code>title</code> and <code>desc</code> are arbitrary free-text from the applicant.</p>

<pre><code class="language-python">df.drop(['pymnt_plan','url','desc','title' ],1, inplace=True)

</code></pre>

<p>Moving on to examine the next slice of columns:</p>

<pre><code class="language-python">df.ix[:5,17:25]
</code></pre>

<p><img src="image5.png" alt="png" /></p>

<h2 id="delinquency">Delinquency</h2>

<p>Lets examine the distribution of delinquency across all applicants:</p>

<pre><code class="language-python">df.delinq_2yrs.value_counts()
</code></pre>

<pre><code>0     186781
1      31859
2       9831
3       3654
4       1606
5        849
6        426
7        254
8        130
9         84
10        47
12        30
11        28
13        21
14         7
15         5
16         5
18         3
19         3
21         2
17         2
22         2
Name: delinq_2yrs, dtype: int64
</code></pre>

<p>Earliest Credit Line
<code>earliest_cr_line</code> is currently an object dtype however we don&rsquo;t want to convert it to a date, rather a scalar to describe the length of time since the first line of credit. We are asserting that if all variables are held equal (number of lines open, income, delinquencies, verfied income); the longer you&rsquo;ve had lines of credit the better.</p>

<p>Conversely an acturary might asses that we can end up with a &ldquo;u-shaped&rdquo; distribution in that if you are the extreme end of years, you are a higher risk as you have a higher chance of mortality thus a lower probability of repayment. Additionally pre-retirement debitors are more likely to list job or health reasons as the primary cause of their bankruptcy.</p>

<p>In a more advanced implementation we&rsquo;d want to account for confounding variables in that certain applicant groups, the older you are (time since first credit line) the larger your earning potential/power and thus the better loan you might secure, however this increase in salary might be negligible if the amount of discretionary money spent or need for a loan scales in some proportion to the salary size.</p>

<pre><code class="language-python">from datetime import datetime

df.earliest_cr_line = pd.to_datetime(df.earliest_cr_line)

dttoday = datetime.now().strftime('%Y-%m-%d')
# There is a better way to do this :) 
df.earliest_cr_line = df.earliest_cr_line.apply(lambda x: (
        np.timedelta64((x - pd.Timestamp(dttoday)),'D').astype(int))/-365)

df.earliest_cr_line
</code></pre>

<pre><code>0         26
1         21
2         13
3         15
4         23
5         12
6         16
7         17
8         14
9         12
10        14
11         6
12        28
13        12
14        20
15        21
16        19
17        26
18        14
19        17
20         9
21        13
22        14
23         8
24         5
25        10
26        13
27        13
28         8
29        21
          ..
235599    15
235600    14
235601    30
235602    22
235603    30
235604    18
235605    18
235606    22
235607    12
235608    17
235609    22
235610    21
235611    13
235612    13
235613    29
235614    15
235615    21
235616    25
235617    14
235618    32
235619    21
235620    22
235621    17
235622    13
235623    16
235624    12
235625    18
235626    12
235627    13
235628    15
Name: earliest_cr_line, dtype: int64
</code></pre>

<p>FICO Ranges
The FICO <code>fico_range_low</code> &amp; <code>fico_range_high</code> scores on their own aren&rsquo;t as useful as a range and average. So let&rsquo;s create that. <code>initial_list_status</code> is unclear if it describes a post or pre determined interest period.</p>

<pre><code class="language-python">df['fico_range'] = df.fico_range_low.astype('str') + '-' + df.fico_range_high.astype('str')
df['meanfico'] = (df.fico_range_low + df.fico_range_high)/2
df.drop(['fico_range_low','fico_range_high','initial_list_status'],1, inplace=True)
</code></pre>

<p>Moving on to the next set of columns</p>

<pre><code class="language-python">df.ix[:5,23:32]
</code></pre>

<p><img src="image6.png" alt="png" /></p>

<h2 id="revolving-utility">Revolving Utility</h2>

<p>Let&rsquo;s do the same thing we did for the interest rate column</p>

<pre><code class="language-python">df.revol_util = pd.Series(df.revol_util).str.replace('%', '').astype(float)
df.ix[:5,23:32]
</code></pre>

<p><img src="image7.png" alt="png" /></p>

<h2 id="more-post-loan-attributes">More Post Loan Attributes</h2>

<p>Similar to some attributes we encountered above, we have more columns to drop that detail attributes about the loan after it was granted, which we don&rsquo;t care about.</p>

<pre><code class="language-python">df.drop(['out_prncp','out_prncp_inv','total_pymnt',
         'total_pymnt_inv','total_rec_prncp', 'grade', 'sub_grade'] ,1, inplace=True)

df.ix[:5,23:32]
</code></pre>

<p><img src="image8.png" alt="png" /></p>

<pre><code class="language-python">df.drop(['total_rec_int','total_rec_late_fee',
         'recoveries','collection_recovery_fee',
         'collection_recovery_fee' ],1, inplace=True)

df.ix[:5,23:32]
</code></pre>

<p><img src="image10.png" alt="png" /></p>

<pre><code class="language-python">df.drop(['last_pymnt_d','last_pymnt_amnt',
         'next_pymnt_d','last_credit_pull_d'],1, inplace=True)

df.ix[:5,23:32]
</code></pre>

<p><img src="image11.png" alt="png" /></p>

<h2 id="last-fico-ranges">Last FICO Ranges</h2>

<p>Our previous FICO attributes were the current ranges.</p>

<pre><code class="language-python">df['last_fico_range'] = df.last_fico_range_low.astype('str') + '-' + df.last_fico_range_high.astype('str')
df['last_meanfico'] = (df.last_fico_range_low + df.last_fico_range_high)/2
df.drop(['last_fico_range_high','last_fico_range_low','policy_code'],1, inplace=True)
</code></pre>

<h2 id="wrapping-up-the-data-munging">Wrapping Up the Data Munging</h2>

<p>Okay so now we got out data down to 61mb from 120mb. Imagine what we could do if we used more efficicent data types! Let&rsquo;s take one last look at it to then move on to our modeling phase.</p>

<pre><code class="language-python">print df.columns
print df.head(1).values
df.info()
</code></pre>

<pre><code>Index([u'loan_amnt', u'funded_amnt', u'funded_amnt_inv', u'term', u'int_rate',
       u'installment', u'emp_length', u'home_ownership', u'annual_inc',
       u'verification_status', u'issue_d', u'purpose', u'zip_code',
       u'addr_state', u'dti', u'delinq_2yrs', u'earliest_cr_line',
       u'inq_last_6mths', u'mths_since_last_delinq', u'mths_since_last_record',
       u'open_acc', u'pub_rec', u'revol_bal', u'revol_util', u'total_acc',
       u'collections_12_mths_ex_med', u'mths_since_last_major_derog',
       u'fico_range', u'meanfico', u'last_fico_range', u'last_meanfico'],
      dtype='object')
[[10400 10400 10400 ' 36 months' 6.99 321.08 8 'MORTGAGE' 58000.0
  'not verified' 'Dec' 'credit_card' '937xx' 'CA' 14.92 0 26 2 42.0 nan 17
  0 6133 31.6 36 0 59.0 '710-714' 712.0 '675-679' 677.0]]
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 235629 entries, 0 to 235628
Data columns (total 31 columns):
loan_amnt                      235629 non-null int64
funded_amnt                    235629 non-null int64
funded_amnt_inv                235629 non-null int64
term                           235629 non-null object
int_rate                       235629 non-null float64
installment                    235629 non-null float64
emp_length                     235629 non-null int64
home_ownership                 235629 non-null object
annual_inc                     235629 non-null float64
verification_status            235629 non-null object
issue_d                        235629 non-null object
purpose                        235629 non-null object
zip_code                       235629 non-null object
addr_state                     235629 non-null object
dti                            235629 non-null float64
delinq_2yrs                    235629 non-null int64
earliest_cr_line               235629 non-null int64
inq_last_6mths                 235629 non-null int64
mths_since_last_delinq         119748 non-null float64
mths_since_last_record         41524 non-null float64
open_acc                       235629 non-null int64
pub_rec                        235629 non-null int64
revol_bal                      235629 non-null int64
revol_util                     235504 non-null float64
total_acc                      235629 non-null int64
collections_12_mths_ex_med     235629 non-null int64
mths_since_last_major_derog    66478 non-null float64
fico_range                     235629 non-null object
meanfico                       235629 non-null float64
last_fico_range                235629 non-null object
last_meanfico                  235629 non-null float64
dtypes: float64(10), int64(12), object(9)
memory usage: 57.5+ MB
</code></pre>

<pre><code class="language-python">df.fillna(0.0,inplace=True)
df.fillna(0,inplace=True)
</code></pre>

<h2 id="our-model">Our model</h2>

<p>Staying true to the original intent of this blog, reproducing my popular old R post, I will use a gradient boosting regression model (more on this later), even if it&rsquo;s easier to use random forest and avoid dummy encoding of my categorical data. If you walk through the broken out steps above you&rsquo;ll find that we have:</p>

<ol>
<li>Addressed missing data</li>
<li>Converted strings to numerical representations where possible</li>
<li>Dropped superfluous attributes</li>
</ol>

<p><strong>Note:</strong> We have to pay special attention to how we replace missing values since we can introduce bias if the data is not missing at random.</p>

<p>The usual next steps are to:</p>

<ol>
<li>Eliminate zero (or near-zero) variance predictors</li>
<li>Highly and correlated predictors</li>
</ol>

<p>Due to all the categorical variables we might want to use Kendall&rsquo;s correlation matrix. If our data was larger we might want to reduce the size of it via more advanced feature selection methods such as clustering (choosing a single representative of each cluster), PCA, ICA, and so on. Since the purpose of this experiment is explanatory rather than predictive, we wouldn&rsquo;t want to use PCA and try to explain the covariates in terms of principal components.</p>

<h3 id="highly-correlated-data">Highly Correlated Data</h3>

<p>Let&rsquo;s examine our dataframes correlation matrix and drop highly correlated/redundant data to address multicollinearity.</p>

<pre><code class="language-python">cor = df.corr()
cor.loc[:,:] = np.tril(cor, k=-1) # below main lower triangle of an array
cor = cor.stack()
cor[(cor &gt; 0.55) | (cor &lt; -0.55)]
</code></pre>

<pre><code>funded_amnt      loan_amnt                 1.000000
funded_amnt_inv  loan_amnt                 0.999997
                 funded_amnt               0.999997
installment      loan_amnt                 0.947978
                 funded_amnt               0.947978
                 funded_amnt_inv           0.947943
pub_rec          mths_since_last_record    0.665077
total_acc        open_acc                  0.684919
dtype: float64
</code></pre>

<p>We will drop the non-informative columns found above. Note that we missed a few columns in our munging process above such as funded_amnt_inv which is the amount funded by customers at that point in time which is post interest rate determination. We also missed installment which is the monthly payment owed by the borrower if the loan originates.</p>

<pre><code class="language-python">df.drop(['zip_code','funded_amnt','funded_amnt_inv', 'installment', 'mths_since_last_delinq', 'total_acc'], axis=1, inplace=True)
</code></pre>

<h3 id="feature-space-and-labels">Feature Space and Labels</h3>

<p>Before we go on any further and verify what we already intuitively know. Let&rsquo;s make a quick (wasn&rsquo;t really quick) plot to examine the relationship between interest rate and FICO score. We also can finish this post without replicating the same image from original post from long ago. Arguably it was a lot more intuitive to do in native ggplot back then.</p>

<pre><code class="language-python">plot_df = df.query('last_meanfico &gt; 600 &amp; int_rate &lt;28')[:3000]

sns.set(font_scale=1.2, rc={&quot;lines.linewidth&quot;: 1.5}) 

g = sns.lmplot(&quot;int_rate&quot;, &quot;last_meanfico&quot;, x_jitter= .7,  y_jitter= .1,
           data=plot_df, hue='term',lowess=True, size=5,aspect=1.4, legend_out=False,
           scatter_kws={ 's':20, 'alpha':.6})

g.set(xlim=(2.5, 28),ylim=(580, 880),alpha = .5)

g.savefig('1.png',transparent=True)
</code></pre>

<p><img src="image0.png" alt="png" /></p>

<pre><code class="language-python">y = df.int_rate.values
df.drop('int_rate',axis = 1, inplace=True)

np.unique(y), pd.Series(y).plot(kind='hist',alpha=.7, bins=20, title='Interest Rate Distribution')
</code></pre>

<pre><code>(array([  6.  ,   6.03,   6.49,   6.62,   6.99,   7.12,   7.49,   7.62,
          7.69,   7.9 ,   8.19,   8.39,   8.67,   8.9 ,   9.17,   9.49,
          9.67,  10.15,  10.49,  10.99,  11.44,  11.67,  11.99,  12.39,
         12.49,  12.85,  12.99,  13.35,  13.53,  13.65,  13.66,  13.98,
         14.16,  14.31,  14.47,  14.49,  14.64,  14.98,  14.99,  15.31,
         15.59,  15.61,  15.99,  16.24,  16.29,  16.49,  16.59,  16.99,
         17.14,  17.57,  17.86,  18.24,  18.25,  18.54,  18.92,  18.99,
         19.2 ,  19.22,  19.24,  19.47,  19.52,  19.97,  19.99,  20.2 ,
         20.49,  20.5 ,  20.99,  21.18,  21.48,  21.99,  22.15,  22.4 ,
         22.45,  22.9 ,  22.99,  23.4 ,  23.43,  23.7 ,  23.99,  24.08,
         24.5 ,  24.99,  25.57,  25.8 ,  25.83,  25.89,  25.99,  26.06]),
</code></pre>

<p><img src="image12.png" alt="png" /></p>

<p>We&rsquo;re about to blow up the feature space by dummy-encoding our categorical variables! This will make our dataframe 8x bgger in memory going from 60 to 470MB.</p>

<pre><code class="language-python">df = pd.get_dummies(df)
df.info()
</code></pre>

<pre><code class="language-&lt;class">Int64Index: 235629 entries, 0 to 235628
Columns: 209 entries, loan_amnt to last_fico_range_845-850
dtypes: float64(200), int64(9)
memory usage: 377.5 MB
</code></pre>

<h2 id="gradient-boosted-regression-trees">Gradient Boosted Regression Trees</h2>

<p>Like the original post we will use GBTs. The tradeofs are listed below:</p>

<p>Advantages</p>

<ul>
<li><strong>Heterogeneous data</strong> - Deatures measured on different scale such as employment length vs annual income</li>
<li><strong>Supports different loss functions</strong> - We can pick a loss function that suites our problem. If our data contains a lot of missing labels like huber loss, or something specific for ranking problems.</li>
<li><strong>Automatically detects (non-linear) feature interactions</strong></li>
</ul>

<p>Disadvantages</p>

<ul>
<li><strong>Requires careful tuning</strong> - RF are faster to tune, they have essentially one parameter</li>
<li><strong>Slow to train</strong> - But fast to predict</li>
<li><strong>Cannot extrapolate</strong> - It is not possible to predict beyond the minimum and maximum limits of the response variable in the training data.</li>
</ul>

<pre><code class="language-python">from sklearn import ensemble
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.metrics import mean_squared_error
from matplotlib import pyplot as plt

X, y = shuffle(df.values, y, random_state=30)
X = X.astype(np.float32)

offset = int(X.shape[0] * 0.75)
X_train, y_train = X[:offset], y[:offset]
X_test, y_test = X[offset:], y[offset:]
</code></pre>

<h2 id="training-tuning">Training &amp; Tuning</h2>

<p>We won&rsquo;t spend too much time in this post tuning, but in general GBTs give us threes knobs we can tune for overfitting: (1) Tree Structure, (2) Shrinkage, (3) Stochastic Gradient Boosting. In the interest of time we&rsquo;ll do a simple grid search amongst a hand chosen set of hyper-parameters.</p>

<p>One of the most effective paramerters to tune for when working with a large feature set is <code>max_features</code> as it introduces a notion of randomization similar to Random Forests. Playing with max features allows us to perform subsampling of our feature space before finding the best split node. A <code>max_features</code> setting of .20 for example would grow each tree on 20% of the featureset. Conversely the subsample feature would use 20% of the training data (all features). Subsample interacts with the parameter n_estimators. Choosing subsample &lt; 1.0 leads to a reduction of variance and an increase in bias.</p>

<h3 id="loss-function">Loss Function</h3>

<ul>
<li>Squared loss minimizes expectation. Pick it when learning expected return on a stock.</li>
<li>Logistic loss minimizes probability. Pick it when learning probability of click on advertisement.</li>
<li>Hinge loss minimizes the 0,1 / yes-no question. Pick it when you want a hard prediction.</li>
<li>Quantile loss minimizes the median. Pick it when you want to predict house prices.</li>
<li>Ensembling multiple loss functions</li>
</ul>

<p>Adapted from: <a href="https://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions">https://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions</a></p>

<pre><code class="language-python">from sklearn.grid_search import GridSearchCV

param_grid = {'learning_rate': [0.1, 0.05, 0.02, 0.01],
              'max_depth': [4, 6],
              'min_samples_leaf': [3, 5, 9, 17],
              'max_features': [1.0, 0.3, 0.1]
              }
# param_grid = {'learning_rate': [0.1],
#               'max_depth': [4],
#               'min_samples_leaf': [3],
#               'max_features': [1.0],
#               }

est = GridSearchCV(ensemble.GradientBoostingRegressor(n_estimators=100),
                   param_grid, n_jobs=4, refit=True)

est.fit(X_train, y_train)

best_params = est.best_params_
</code></pre>

<pre><code class="language-python">%%time
est = ensemble.GradientBoostingRegressor(n_estimators=2000).fit(X_train, y_train)
</code></pre>

<p><code>CPU times: user 2h 15min 34s, sys: 5min 43s, total: 2h 21min 18s</code><br />
<code>Wall time: 3h 28min 36s</code></p>

<pre><code class="language-python">est.score(X_test,y_test)
</code></pre>

<p><code>.7644790837986264</code></p>

<h2 id="examing-results">Examing results</h2>

<p>In terms of simple model accuracy we know we&rsquo;re doing pretty good with the above score over 3000 iterations gets us in the 90s, 1000 gets us in the 60s.Let&rsquo;s examine how drastically accuracy increased with each iteration and see which model features were most powerful in predicting the issued iterest rate.</p>

<pre><code class="language-python">sns.set(font_scale=1, rc={&quot;lines.linewidth&quot;:1.2}) 


Iterations = 2000
# compute test set deviance
test_score = np.zeros((Iterations,), dtype=np.float64)

for i, y_pred in enumerate(est.staged_predict(X_test)):
    test_score[i] = est.loss_(y_test, y_pred)

plt.figure(figsize=(14, 6)).subplots_adjust(wspace=.3)

plt.subplot(1, 2, 1)
plt.title('Deviance over iterations')
plt.plot(np.arange(Iterations) + 1, est.train_score_, 'dodgerblue',
         label='Training Set Deviance', alpha=.6)
plt.plot(np.arange(Iterations) + 1, test_score, 'firebrick',
         label='Test Set Deviance', alpha=.6)
plt.legend(loc='upper right')
plt.xlabel('Boosting Iterations')
plt.ylabel('Deviance')


plt.subplot(1, 2, 2,)
# Top Ten
feature_importance = est.feature_importances_
feature_importance = 100.0 * (feature_importance / feature_importance.max())

indices = np.argsort(feature_importance)[-10:]
plt.barh(np.arange(10), feature_importance[indices],color='dodgerblue',alpha=.4)
plt.yticks(np.arange(10 + 0.25), np.array(df.columns)[indices])
_ = plt.xlabel('Relative importance'), plt.title('Top Ten Important Variables')
</code></pre>

<p><img src="image13.png" alt="png" /></p>

<h2 id="partial-dependence-plots">Partial Dependence Plots</h2>

<p>When working with tree models such as GBTs or Random Forests, I like to take a look at Partial Dependence Plots to understand the functional relations between predictors and an outcome. Tese plots capture marginal effect of a given variable or variables on the target function, in this case interest rate.</p>

<p>In the first plot, tt is interesting to note that below $25k annual income, Debt-to-Income ratio has a large effect on the interest rate. The margin gets significantly larger after 75k. The steepness of meanfico is especially interesting, it looks like no real distinction is made for customers with a score over 760.</p>

<pre><code class="language-python">from sklearn.ensemble.partial_dependence import plot_partial_dependence

comp_features = [('annual_inc','dti'),'loan_amnt','meanfico','annual_inc', 'inq_last_6mths', 'revol_util', 'dti']

fig, axs = plot_partial_dependence(est, X_train, comp_features,
                                   feature_names=list(df.columns),
                                   figsize=(14, 14), n_jobs=4)
</code></pre>

<p><img src="image14.png" alt="png" /></p>

<p>Trying out some alternate plots from seaborn.</p>

<pre><code class="language-python">%%time
sns.jointplot(y,df.meanfico.values,annot_kws=dict(stat=&quot;r&quot;),
              kind=&quot;kde&quot;, color=&quot;#4CB391&quot;).set_axis_labels(&quot;int_rate&quot;, &quot;mean_fico&quot;)
</code></pre>

<p><code>CPU times: user 2min 3s, sys: 1.14 s, total: 2min 5s</code><br />
<code>Wall time: 2min 6s</code></p>

<p><img src="image15.png" alt="png" /></p>

  </main>

          <footer role="contentinfo">
  <div>
    <label for="themer">
      dark theme: <input type="checkbox" id="themer" class="vh">
      <span aria-hidden="true"></span>
    </label>
  </div>
  
    Made with <a href="https://gohugo.io/">Hugo</a>. Deployed to <a href="https://app.netlify.com/">Netlify</a>. Source on <a href="https://github.com/kevindavenport/kldavenport.com">Github</a>.
  
</footer>

        </div>
      </div>
    </div>
    <script src="https://kldavenport.netlify.com/js/prism.js"></script>
<script src="https://kldavenport.netlify.com/js/dom-scripts.js"></script>

    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-34706513-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>
