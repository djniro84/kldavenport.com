<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.54.0" />
  <link rel="canonical" href="https://kldavenport.netlify.com/logistic-regression-intuition/">

  

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://kldavenport.netlify.com/css/prism.css" media="none" onload="this.media='all';">

  
  
  <link rel="stylesheet" type="text/css" href="https://kldavenport.netlify.com/css/styles.css">

  <style id="inverter" media="none">
    html { filter: invert(100%) }
    * { background-color: inherit }
    img:not([src*=".svg"]), .colors, iframe, .demo-container { filter: invert(100%) }
  </style>

  
  
  <title>Understanding Logistic Regression Intuitively | Kevin Davenport Engineering &amp; ML blog</title>
</head>

  <body>
    <a href="#main">skip to content</a>
    <svg style="display: none">
  <symbol id="bookmark" viewBox="0 0 40 50">
   <g transform="translate(2266 3206.2)">
    <path style="stroke:currentColor;stroke-width:3.2637;fill:none" d="m-2262.2-3203.4-.2331 42.195 16.319-16.318 16.318 16.318.2331-42.428z"/>
   </g>
  </symbol>

  <symbol id="w3c" viewBox="0 0 127.09899 67.763">
   <text font-size="83" style="font-size:83px;font-family:Trebuchet;letter-spacing:-12;fill-opacity:0" letter-spacing="-12" y="67.609352" x="-26.782778">W3C</text>
   <text font-size="83" style="font-size:83px;font-weight:bold;font-family:Trebuchet;fill-opacity:0" y="67.609352" x="153.21722" font-weight="bold">SVG</text>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m33.695.377 12.062 41.016 12.067-41.016h8.731l-19.968 67.386h-.831l-12.48-41.759-12.479 41.759h-.832l-19.965-67.386h8.736l12.061 41.016 8.154-27.618-3.993-13.397h8.737z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m91.355 46.132c0 6.104-1.624 11.234-4.862 15.394-3.248 4.158-7.45 6.237-12.607 6.237-3.882 0-7.263-1.238-10.148-3.702-2.885-2.47-5.02-5.812-6.406-10.022l6.82-2.829c1.001 2.552 2.317 4.562 3.953 6.028 1.636 1.469 3.56 2.207 5.781 2.207 2.329 0 4.3-1.306 5.909-3.911 1.609-2.606 2.411-5.738 2.411-9.401 0-4.049-.861-7.179-2.582-9.399-1.995-2.604-5.129-3.912-9.397-3.912h-3.327v-3.991l11.646-20.133h-14.062l-3.911 6.655h-2.493v-14.976h32.441v4.075l-12.31 21.217c4.324 1.385 7.596 3.911 9.815 7.571 2.22 3.659 3.329 7.953 3.329 12.892z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.21 0 1.414 8.6-5.008 9.583s-1.924-4.064-5.117-6.314c-2.693-1.899-4.447-2.309-7.186-1.746-3.527.73-7.516 4.938-9.258 10.13-2.084 6.21-2.104 9.218-2.178 11.978-.115 4.428.58 7.043.58 7.043s-3.04-5.626-3.011-13.866c.018-5.882.947-11.218 3.666-16.479 2.404-4.627 5.954-7.404 9.114-7.728 3.264-.343 5.848 1.229 7.841 2.938 2.089 1.788 4.213 5.698 4.213 5.698l4.94-9.837z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.82 48.674s-2.208 3.957-3.589 5.48c-1.379 1.524-3.849 4.209-6.896 5.555-3.049 1.343-4.646 1.598-7.661 1.306-3.01-.29-5.807-2.032-6.786-2.764-.979-.722-3.486-2.864-4.897-4.854-1.42-2-3.634-5.995-3.634-5.995s1.233 4.001 2.007 5.699c.442.977 1.81 3.965 3.749 6.572 1.805 2.425 5.315 6.604 10.652 7.545 5.336.945 9.002-1.449 9.907-2.031.907-.578 2.819-2.178 4.032-3.475 1.264-1.351 2.459-3.079 3.116-4.108.487-.758 1.276-2.286 1.276-2.286l-1.276-6.644z"/>
  </symbol>

  <symbol id="tag" viewBox="0 0 177.16535 177.16535">
    <g transform="translate(0 -875.2)">
     <path style="fill-rule:evenodd;stroke-width:0;fill:currentColor" d="m159.9 894.3-68.79 8.5872-75.42 77.336 61.931 60.397 75.429-76.565 6.8495-69.755zm-31.412 31.835a10.813 10.813 0 0 1 1.8443 2.247 10.813 10.813 0 0 1 -3.5174 14.872l-.0445.0275a10.813 10.813 0 0 1 -14.86 -3.5714 10.813 10.813 0 0 1 3.5563 -14.863 10.813 10.813 0 0 1 13.022 1.2884z"/>
    </g>
  </symbol>

  <symbol id="balloon" viewBox="0 0 141.73228 177.16535">
   <g transform="translate(0 -875.2)">
    <g>
     <path style="fill:currentColor" d="m68.156 882.83-.88753 1.4269c-4.9564 7.9666-6.3764 17.321-5.6731 37.378.36584 10.437 1.1246 23.51 1.6874 29.062.38895 3.8372 3.8278 32.454 4.6105 38.459 4.6694-.24176 9.2946.2879 14.377 1.481 1.2359-3.2937 5.2496-13.088 8.886-21.623 6.249-14.668 8.4128-21.264 10.253-31.252 1.2464-6.7626 1.6341-12.156 1.4204-19.764-.36325-12.93-2.1234-19.487-6.9377-25.843-2.0833-2.7507-6.9865-7.6112-7.9127-7.8436-.79716-.20019-6.6946-1.0922-6.7755-1.0248-.02213.0182-5.0006-.41858-7.5248-.22808l-2.149-.22808h-3.3738z"/>
     <path style="fill:currentColor" d="m61.915 883.28-3.2484.4497c-1.7863.24724-3.5182.53481-3.8494.63994-2.4751.33811-4.7267.86957-6.7777 1.5696-.28598 0-1.0254.20146-2.3695.58589-5.0418 1.4418-6.6374 2.2604-8.2567 4.2364-6.281 7.6657-11.457 18.43-12.932 26.891-1.4667 8.4111.71353 22.583 5.0764 32.996 3.8064 9.0852 13.569 25.149 22.801 37.517 1.3741 1.841 2.1708 2.9286 2.4712 3.5792 3.5437-1.1699 6.8496-1.9336 10.082-2.3263-1.3569-5.7831-4.6968-21.86-6.8361-33.002-.92884-4.8368-2.4692-14.322-3.2452-19.991-.68557-5.0083-.77707-6.9534-.74159-15.791.04316-10.803.41822-16.162 1.5026-21.503 1.4593-5.9026 3.3494-11.077 6.3247-15.852z"/>
     <path style="fill:currentColor" d="m94.499 885.78c-.10214-.0109-.13691 0-.0907.0409.16033.13489 1.329 1.0675 2.5976 2.0723 6.7003 5.307 11.273 14.568 12.658 25.638.52519 4.1949.24765 14.361-.5059 18.523-2.4775 13.684-9.7807 32.345-20.944 53.519l-3.0559 5.7971c2.8082.76579 5.7915 1.727 8.9926 2.8441 11.562-11.691 18.349-19.678 24.129-28.394 7.8992-11.913 11.132-20.234 12.24-31.518.98442-10.02-1.5579-20.876-6.7799-28.959-.2758-.4269-.57803-.86856-.89617-1.3166-3.247-6.13-9.752-12.053-21.264-16.131-2.3687-.86369-6.3657-2.0433-7.0802-2.1166z"/>
     <path style="fill:currentColor" d="m32.52 892.22c-.20090-.13016-1.4606.81389-3.9132 2.7457-11.486 9.0476-17.632 24.186-16.078 39.61.79699 7.9138 2.4066 13.505 5.9184 20.562 5.8577 11.77 14.749 23.219 30.087 38.74.05838.059.12188.1244.18052.1838 1.3166-.5556 2.5965-1.0618 3.8429-1.5199-.66408-.32448-1.4608-1.3297-3.8116-4.4602-5.0951-6.785-8.7512-11.962-13.051-18.486-5.1379-7.7948-5.0097-7.5894-8.0586-13.054-6.2097-11.13-8.2674-17.725-8.6014-27.563-.21552-6.3494.13041-9.2733 1.775-14.987 2.1832-7.5849 3.9273-10.986 9.2693-18.07 1.7839-2.3656 2.6418-3.57 2.4409-3.7003z"/>
     <path style="fill:currentColor" d="m69.133 992.37c-6.2405.0309-12.635.76718-19.554 2.5706 4.6956 4.7759 9.935 10.258 12.05 12.625l4.1272 4.6202h11.493l3.964-4.4516c2.0962-2.3541 7.4804-7.9845 12.201-12.768-8.378-1.4975-16.207-2.6353-24.281-2.5955z"/>
     <rect style="stroke-width:0;fill:currentColor" ry="2.0328" height="27.746" width="22.766" y="1017.7" x="60.201"/>
    </g>
   </g>
  </symbol>

  <symbol id="info" viewBox="0 0 41.667 41.667">
   <g transform="translate(-37.035 -1004.6)">
    <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m76.25 1030.2a18.968 18.968 0 0 1 -23.037 13.709 18.968 18.968 0 0 1 -13.738 -23.019 18.968 18.968 0 0 1 23.001 -13.768 18.968 18.968 0 0 1 13.798 22.984"/>
    <g transform="matrix(1.1146 0 0 1.1146 -26.276 -124.92)">
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m75.491 1039.5v-8.7472"/>
     <path style="stroke-width:0;fill:currentColor" transform="scale(-1)" d="m-73.193-1024.5a2.3719 2.3719 0 0 1 -2.8807 1.7142 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
   </g>
  </symbol>

  <symbol id="warning" viewBox="0 0 48.430474 41.646302">
    <g transform="translate(-1.1273 -1010.2)">
     <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:4.151;fill:none" d="m25.343 1012.3-22.14 37.496h44.28z"/>
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:4.1512;fill:none" d="m25.54 1027.7v8.7472"/>
     <path style="stroke-width:0;fill:currentColor" d="m27.839 1042.8a2.3719 2.3719 0 0 1 -2.8807 1.7143 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
  </symbol>

  <symbol id="menu" viewBox="0 0 50 50">
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="0" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="20" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="40" x="0"/>
   </symbol>

   <symbol id="link" viewBox="0 0 50 50">
    <g transform="translate(0 -1002.4)">
     <g transform="matrix(.095670 0 0 .095670 2.3233 1004.9)">
      <g>
       <path style="stroke-width:0;fill:currentColor" d="m452.84 192.9-128.65 128.65c-35.535 35.54-93.108 35.54-128.65 0l-42.881-42.886 42.881-42.876 42.884 42.876c11.845 11.822 31.064 11.846 42.886 0l128.64-128.64c11.816-11.831 11.816-31.066 0-42.9l-42.881-42.881c-11.822-11.814-31.064-11.814-42.887 0l-45.928 45.936c-21.292-12.531-45.491-17.905-69.449-16.291l72.501-72.526c35.535-35.521 93.136-35.521 128.64 0l42.886 42.881c35.535 35.523 35.535 93.141-.001 128.66zm-254.28 168.51-45.903 45.9c-11.845 11.846-31.064 11.817-42.881 0l-42.884-42.881c-11.845-11.821-11.845-31.041 0-42.886l128.65-128.65c11.819-11.814 31.069-11.814 42.884 0l42.886 42.886 42.876-42.886-42.876-42.881c-35.54-35.521-93.113-35.521-128.65 0l-128.65 128.64c-35.538 35.545-35.538 93.146 0 128.65l42.883 42.882c35.51 35.54 93.11 35.54 128.65 0l72.496-72.499c-23.956 1.597-48.092-3.784-69.474-16.283z"/>
      </g>
     </g>
    </g>
  </symbol>

  <symbol id="doc" viewBox="0 0 35 45">
   <g transform="translate(-147.53 -539.83)">
    <path style="stroke:currentColor;stroke-width:2.4501;fill:none" d="m149.38 542.67v39.194h31.354v-39.194z"/>
    <g style="stroke-width:25" transform="matrix(.098003 0 0 .098003 133.69 525.96)">
     <path d="m220 252.36h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path style="stroke:currentColor;stroke-width:25;fill:none" d="m220 409.95h200"/>
     <path d="m220 488.74h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path d="m220 331.15h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
    </g>
   </g>
 </symbol>

 <symbol id="tick" viewBox="0 0 177.16535 177.16535">
  <g transform="translate(0 -875.2)">
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="155" width="40" y="702.99" x="556.82"/>
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="40" width="90.404" y="817.99" x="506.42"/>
  </g>
 </symbol>
</svg>

    <div class="wrapper">
      <header class="intro-and-nav" role="banner">
  <div>
    <div class="intro">
      <a class="logo" href="/" aria-label="Kevin Davenport Engineering &amp; ML blog home page">
        <img src="https://kldavenport.netlify.com/images/logo.svg" alt="">
      </a>
      <p class="library-desc">
        
      </p>
    </div>
    <nav id="patterns-nav" class="patterns" role="navigation">
  <h2 class="vh">Main navigation</h2>
  <button id="menu-button" aria-expanded="false">
    <svg viewBox="0 0 50 50" aria-hidden="true" focusable="false">
      <use xlink:href="#menu"></use>
    </svg>
    Menu
  </button>
  
  <ul id="patterns-list">
  
    <li class="pattern">
      
      
      
      
      <a href="/post/" aria-current="page">
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Blog</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/about/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">About</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/bike-travel/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Bike Travel</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/tags/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Tags</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/index.xml" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">RSS</span>
      </a>
    </li>
  
  </ul>
</nav>
  </div>
</header>
      <div class="main-and-footer">
        <div>
          
  <main id="main">
    <h1>
      <svg class="bookmark-icon" aria-hidden="true" viewBox="0 0 40 50" focusable="false">
        <use xlink:href="#bookmark"></use>
      </svg>
      Understanding Logistic Regression Intuitively
    </h1>
    <div class="date">
      
      <strong aria-hidden="true">Publish date: </strong>Oct 17, 2014
    </div>
    
      <div class="tags">
        <strong aria-hidden="true">Tags: </strong>
        <ul aria-label="tags">
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/logistic-regression">logistic regression</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/machine-learning">machine learning</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.netlify.com/tags/supervised-learning">supervised learning</a>
            </li>
          
        </ul>
      </div>
    
    
    
      

  <nav class="toc" aria-labelledby="toc-heading">
    <h2 id="toc-heading">Table of contents</h2>
    <ol>
      
        <li>
          <a href="#loading-the-data">
            Loading the data
          </a>
        </li>
      
        <li>
          <a href="#visualizing-the-data">
            Visualizing the data
          </a>
        </li>
      
        <li>
          <a href="#feature-engineering">
            Feature Engineering:
          </a>
        </li>
      
        <li>
          <a href="#finding-the-optimal-theta">
            Finding the optimal theta
          </a>
        </li>
      
        <li>
          <a href="#regularized-logistic-regression-with-scikit-learn">
            Regularized Logistic Regression with scikit-learn
          </a>
        </li>
      
        <li>
          <a href="#loading-data">
            Loading data
          </a>
        </li>
      
        <li>
          <a href="#fitting-and-scoring-the-regularized-logistic-regression-model">
            Fitting and scoring the Regularized Logistic Regression model
          </a>
        </li>
      
    </ol>
  </nav>


    
    

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy import optimize
import pandas as pd
import seaborn as sns
from ggplot import *
%matplotlib inline
</code></pre>

<h2 id="loading-the-data">Loading the data</h2>

<pre><code class="language-python"># Load training data
data = pd.read_csv('ex2data2.txt',header=None, names = ('x1','x2','y'))
data.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 118 entries, 0 to 117
Data columns (total 3 columns):
x1    118 non-null float64
x2    118 non-null float64
y     118 non-null int64
dtypes: float64(2), int64(1)
memory usage: 3.7 KB
</code></pre>

<p>Columns 1 &amp; 2 are the training data, 3 is the label set.</p>

<pre><code class="language-python">data.head(2)
</code></pre>

<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> 0.051267</td>
      <td> 0.69956</td>
      <td> 1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.092742</td>
      <td> 0.68494</td>
      <td> 1</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python"># Get the df values into numpy arrays
X = data.ix[:,0:2].values
y = data.ix[:,2].values
y = y[:, None]
</code></pre>

<h2 id="visualizing-the-data">Visualizing the data</h2>

<p>On almost every release Pandas adds more and more helpful plot methods that help you avoid specifying your plot in matplot directly. These functions are very handy for exploration. I still find myself reaching for ggplot though, perhaps it&rsquo;s because I started in R years ago, or maybe it&rsquo;s because I subscribed to some of the concepts in Wilkinson&rsquo;s <a href="http://www.amazon.com/The-Grammar-Graphics-Statistics-Computing/dp/0387245448">The Grammar of Graphics</a>.</p>

<pre><code class="language-python">data.plot(kind='scatter', x=0, y=1, c = 2, s=40, title='Microchip Test 1 &amp; 2',
          figsize=None, legend= True)
# pd.scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')
</code></pre>

<p><img src="output_8_1.png" alt="png" /></p>

<p>Here&rsquo;s another quick plot with the excellent seaborn package. The syntax is arbitrary across the various plot types unlike ggplot, but it&rsquo;s constantly improving.</p>

<pre><code class="language-python">sns.lmplot('x1', 'x2', hue='y', data=data, fit_reg=False)
</code></pre>

<p><img src="output_10_1.png" alt="png" /></p>

<p>One more take with ggplot. If anyone reading this knows a smarter way of adjusting the plot size please let me know.</p>

<pre><code class="language-python">ggplot(data ,aes(x='x1', y='x2', color='y')) +\
    geom_point(alpha=.7, size = 30) +\
    theme_matplotlib(rc={&quot;figure.figsize&quot;: &quot;5, 5&quot;})
</code></pre>

<p><img src="output_12_0.png" alt="png" /></p>

<p>As we stated before we&rsquo;ll use logistic regression for binary classification. This means we&rsquo;ll be taking a continuous range of input and convert it to either 0 or 1 based on it&rsquo;s range:<br></p>

<p>$h<em>\theta(x)$ = our Logistic Regression function.<br>
If the output of $h</em>\theta(x)\geq .5$, then 1<br>
If the output of $h_\theta(x)\lt .5$, then 0<br></p>

<p>Recall that we&rsquo;ll have a vector of coefficients $\theta$ that we chose to get us as close to y (our labels) as possible, that is to say choose $\theta_0$, $\theta_1$, $\theta_2$ (assuming we only have three coefficients) that minimize the sum of the squared differences (squared error).</p>

<p><img src="http://kldavenport.com/wp-content/uploads/2014/11/logisitic_regression_blog2.png" alt="caption" title="Matrix Examples" /></p>

<p><br>
So it&rsquo;s a clean linear algebra process of: <strong>Prediction = Data * Parameters</strong></p>

<p>Now we&rsquo;ll take our data and process it through the sigmoid function:
$$h_\theta(x) = g(\theta^Tx): g(z) = \frac{1}{1+e^{-z}}$$</p>

<p>The logistic regression cost function including regularization (last term) is:</p>

<p>$$J(\theta) = \frac{1}{m}\sum\limits<em>{i=1}^{m} [-y^{(i)}log(h</em>\theta(x^{(i)})-(1 - y^{(i)})log(1 - h<em>\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum\limits</em>{j=1}^{n}\theta^2_j$$</p>

<p>The gradient of the cost including regularization is:</p>

<p>$$\frac{\partial{J(\theta)}}{\partial{\theta<em>j}} = \frac{1}{m}\sum\limits</em>{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}     \qquad \qquad \qquad   for \,\, j=0 $$</p>

<p>$$\frac{\partial{J(\theta)}}{\partial{\theta<em>j}} = (\frac{1}{m}\sum\limits</em>{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}) + \frac{\lambda}{m}\theta_j    \qquad    for \,\, j \geq 1 $$</p>

<h2 id="feature-engineering">Feature Engineering:</h2>

<p>We can tell from the two-dimensional plots above that we have non-linearly separable data. We might want to start with seeing what kind of boundaries we can train using interactions between the two variables in our feature space. Since we&rsquo;re using logistic regression we&rsquo;ll need to manually construct these new features. This is not the case with more advanced non-linear methods such as SVMs with a Gaussian or polynomial kernel as the various interactions between the input variables are inherently captured by the structure of the model.</p>

<p>Remember that depending on the scale of the new features we create (f.g. X1, X2, X1^2, X2^2, X1*X2, X1*X2^2), we might need to scale down variables to speed up convergence of the gradient descent. Logistic Regression, like SVMs can also suffer poor performance if some input variables have a larger range of values compared to others. Normalization is used to address this: <a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing">http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing</a>.</p>

<pre><code class="language-python">def new_features(X1, X2, degree=18):
    &quot;&quot;&quot;
    Returns a m x n matrix of the original inputs along with new columns based on their exponents and combinations.
    Args:
        x1, x2: 2D arrays (floats) with identical shapes
        highest_poly_degree: int to specify the max polynomial
    &quot;&quot;&quot;    
    num_features = (degree + 1) * (degree + 2) / 2
    out = np.empty(shape=(X1.shape[0], num_features), dtype=float)
    k = 0
    for i in xrange(0, degree + 1):
        for j in xrange(i + 1):
            new_feature_values = X1**(i - j) * X2**j
            out[:, k] = new_feature_values[:, 0]
            k += 1
    return out

X = new_features(X[:, 0][:, None], X[:, 1][:, None])
</code></pre>

<p>$g(z) = \frac{1}{1+e^{-z}}$ expressed as:</p>

<pre><code class="language-python">def sigmoid(z):
    &quot;&quot;&quot;
    Return Sigmoid of z
    Args:
        Z : array or scalar
    &quot;&quot;&quot;
    return 1.0 / (1.0 + np.exp(-z))
</code></pre>

<p>Usually we&rsquo;d want to use a package function. Notice above that we are using numpy&rsquo;s exp and not the math library&rsquo;s exp. There is a common performance hit in Python code of accidentally bouncing back and forth between NumPy and standard functions which are not optimized for the contiguous C arrays of NumPy.</p>

<p>Below we are using SciPy&rsquo;s expit over scipy.stats.logistic.cdf because it is the source operator and not a wrapper function with extra parameters like logistic.cdf. Note the 4x speedup between our manual and SciPy function.</p>

<pre><code class="language-python">from scipy.special import expit
</code></pre>

<pre><code class="language-python">%%timeit 

expit(X[1])
</code></pre>

<p><code>100000 loops, best of 3: 3 µs per loop</code></p>

<pre><code class="language-python">%%timeit

sigmoid(X[1])
</code></pre>

<p><code>100000 loops, best of 3: 6.69 µs per loop</code></p>

<p>$g(\theta^Tx)$ expressed as:</p>

<pre><code class="language-python">def h_of_theta(theta, X):
    transposed_theta = theta[:, None]
    return sigmoid(X.dot(transposed_theta))
</code></pre>

<p>$J(\theta) = \frac{1}{m}\sum\limits<em>{i=1}^{m} [-y^{(i)}log(h</em>\theta(x^{(i)})-(1 - y^{(i)})log(1 - h<em>\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum\limits</em>{j=1}^{n}\theta^2_j$ <br></p>

<p>Recall that the $\frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta^2_j$ at the end of the line above is the regularization term.<br>
expressed as:</p>

<pre><code class="language-python">def cost_function_reg(theta, X, y, lamda):
    &quot;&quot;&quot;
    Return cost of given theta for logistic regression
    Args:
        theta is a 1D numpy array 
        X is a m x n numpy array w/ the first column being an intercept of 1
        y is a m x 1 numpy array, it is the label set for each obs in X
        lambda : float is regularization parameter
    &quot;&quot;&quot;
    m = len(y) * 1.0
    h_theta = h_of_theta(theta, X)


    tol = .00000000000000000000001  
    h_theta[h_theta &lt; tol] = tol  # values close to zero are set to tol
    h_theta[(h_theta &lt; 1 + tol) &amp; (h_theta &gt; 1 - tol)] = 1 - tol  # values close to 1 get set to 1 - tol

    regularization_term = (float(lamda)/2) * theta**2
    cost_vector = y * np.log(h_theta) + (-y + 1) * np.log(-h_theta + 1)

    J = -sum(cost_vector)/m + sum(regularization_term[1:])/m

    return J[0]
</code></pre>

<p>$\frac{\partial{J(\theta)}}{\partial{\theta<em>j}} = \frac{1}{m}\sum\limits</em>{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}     \qquad \qquad \qquad   for \,\, j=0 $</p>

<p>$\frac{\partial{J(\theta)}}{\partial{\theta<em>j}} = (\frac{1}{m}\sum\limits</em>{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}) + \frac{\lambda}{m}\theta_j    \qquad    for \,\, j \geq 1 $ expressed as:</p>

<pre><code class="language-python">def gradient(theta, X, y, lamda):
    &quot;&quot;&quot;
    Return gradient of theta 
    Args:
        theta is a 1D numpy array 
        X is a m x n numpy array w/ the first column being an intercept of 1
        y is a m x 1 numpy array, it is the label set for each obs in X
        lambda : float is regularization parameter
    &quot;&quot;&quot;
    m = len(y)
    h_theta = h_of_theta(theta, X)
    derivative_regularization_term = float(lamda) * theta/m
    grad = (h_theta - y).T.dot(X)/m + derivative_regularization_term.T
    grad[0][0] -= derivative_regularization_term[0]  
    
    return np.ndarray.flatten(grad)  
</code></pre>

<p>Let&rsquo;s see what we have so far and test one iteration of regularized logistic regression at initial theta.</p>

<pre><code class="language-python">m, n = X.shape
initial_theta = np.zeros(n)
lambda2 = 1

cost = cost_function_reg(initial_theta, X, y, lambda2)
grad = gradient(initial_theta, X, y, lambda2)

print 'Training set size:', X.shape
print ''
print 'Cost at initial theta of zeros:', cost
print '\nGradient at initial theta (zeros):\n'
print grad
</code></pre>

<pre><code>Training set size: (118, 190)

Cost at initial theta of zeros: 0.69314718056

Gradient at initial theta (zeros):

[  8.47457627e-03   1.87880932e-02   7.77711864e-05   5.03446395e-02
   1.15013308e-02   3.76648474e-02   1.83559872e-02   7.32393391e-03
   8.19244468e-03   2.34764889e-02   3.93486234e-02   2.23923907e-03
   1.28600503e-02   3.09593720e-03   3.93028171e-02   1.99707467e-02
   4.32983232e-03   3.38643902e-03   5.83822078e-03   4.47629067e-03
   3.10079849e-02   3.10312442e-02   1.09740238e-03   6.31570797e-03
   4.08503006e-04   7.26504316e-03   1.37646175e-03   3.87936363e-02
   1.99427086e-02   2.74239681e-03   2.32500787e-03   2.21859968e-03
   1.37054473e-03   4.59059364e-03   2.44887343e-03   3.45775396e-02
   2.58470812e-02   9.24073316e-04   3.56449986e-03   2.36431492e-04
   2.73345994e-03   1.41372690e-04   5.06220460e-03   7.44299593e-04
   3.91158180e-02   1.94460178e-02   2.03971305e-03   1.67313910e-03
   9.57374855e-04   8.28510751e-04   1.45754709e-03   5.55440822e-04
   3.80878101e-03   1.42922173e-03   3.74400380e-02   2.27382094e-02
   1.00011095e-03   2.22706585e-03   1.51733578e-04   1.25745954e-03
   1.48580695e-04   1.57570852e-03   3.59508725e-06   3.99563605e-03
   5.22308844e-04   4.06790884e-02   1.90410565e-02   1.75081954e-03
   1.23641700e-03   4.69990556e-04   5.27414397e-04   5.55662823e-04
   3.40580487e-04   1.05188919e-03   1.97028168e-04   3.35794866e-03
   9.67859736e-04   4.06730521e-02   2.10163086e-02   1.15032574e-03
   1.49782866e-03   1.02669187e-04   6.64345919e-04   1.09951984e-04
   6.18955072e-04   7.43336884e-05   1.07123799e-03  -6.55122093e-05
   3.44743950e-03   5.15509782e-04   4.35307118e-02   1.89437400e-02
   1.66248899e-03   9.41574445e-04   2.61308468e-04   3.41532634e-04
   2.43492082e-04   2.14664106e-04   3.63362793e-04   1.41628227e-04
   8.21956083e-04   3.56919486e-05   3.11923871e-03   8.32085947e-04
   4.47399893e-02   2.02778726e-02   1.31985563e-03   1.06764143e-03
   7.86844535e-05   3.87047105e-04   6.94079048e-05   2.85477158e-04
   6.90699995e-05   3.69582263e-04   2.39341381e-05   8.16074446e-04
  -9.08769094e-05   3.17917837e-03   6.45964381e-04   4.77107504e-02
   1.92318272e-02   1.68027441e-03   7.40076535e-04   1.63801867e-04
   2.27678507e-04   1.19804339e-04   1.31380821e-04   1.46616000e-04
   9.85447536e-05   2.59049209e-04   4.94761874e-05   6.85844461e-04
  -3.01668071e-05   3.02344543e-03   9.01454727e-04   4.99273765e-02
   2.02879133e-02   1.49547626e-03   8.00908230e-04   6.71510752e-05
   2.41544808e-04   4.36257990e-05   1.48661051e-04   4.47256584e-05
   1.50975684e-04   3.97448551e-05   2.53079620e-04  -7.50732189e-06
   6.74686175e-04  -8.71168306e-05   3.08384121e-03   8.78361332e-04
   5.33211342e-02   1.99316045e-02   1.76219133e-03   6.01669008e-04
   1.14605446e-04   1.56266718e-04   6.51050016e-05   8.16103375e-05
   6.70731769e-05   5.97739840e-05   9.62532096e-05   4.76882683e-05
   1.98939294e-04   2.73386602e-06   6.02466949e-04  -4.37233090e-05
   3.03428354e-03   1.11286804e-03   5.64868812e-02   2.09115586e-02
   1.67875987e-03   6.31032018e-04   6.09263266e-05   1.58379301e-04
   2.87976108e-05   8.46763041e-05   2.69063871e-05   7.05747985e-05
   2.76208043e-05   9.30497298e-05   2.08125233e-05   1.91484135e-04
  -2.66222573e-05   5.92085411e-04  -6.18760518e-05   3.11056877e-03
   1.19614065e-03   6.05363861e-02]
</code></pre>

<h2 id="finding-the-optimal-theta">Finding the optimal theta</h2>

<p><a href="http://docs.scipy.org/doc/scipy-0.14.0/reference/optimize.html">scipy.optimize</a> contains SciPy&rsquo;s the optimization and root finding methods. In this instance we&rsquo;ll use fmin_bfgs to minimize our function using the BFGS algorithm. I&rsquo;m no math PhD, but what we&rsquo;re doing with BFGS is approximating Newton&rsquo;s method, which is an iterative method applied to the derivative (slope) of a function to find zeros. As you can see we&rsquo;re digging just a little deeper than simply calling <code>fit, predict, score, transform</code> in sci-kit learn.</p>

<pre><code class="language-python"># set options then run fmin_bfgs to obtain optimal theta (BFGS: quasi-Newton method of Broyden, Fletcher, Goldfarb, and Shanno)
initial_theta = np.zeros(n)
lambda2 = 1
myargs = (X, y, lambda2)
opts = {'full_output': True, 'maxiter': 400}

optimal_theta, cost, grad_at_min, inv_hessian_matrix,\
fun_calls, grad_calls, warn_flags = optimize.fmin_bfgs(cost_function_reg,
                                initial_theta,
                                args=myargs,
                                fprime=gradient,
                                **opts)

print '\nCost at theta found by fmin_bfgs:', cost
print '\noptimal theta:'
print optimal_theta
</code></pre>

<pre><code>Optimization terminated successfully.
         Current function value: 0.518005
         Iterations: 53
         Function evaluations: 54
         Gradient evaluations: 54

Cost at theta found by fmin_bfgs: 0.518004571479

optimal theta:
[  1.23754420e+00   6.48852298e-01   1.16024020e+00  -1.89123994e+00
  -9.12464294e-01  -1.36430785e+00   2.71970488e-01  -3.83457368e-01
  -3.90662110e-01  -4.67405933e-03  -1.27271558e+00  -4.47942003e-02
  -6.38674615e-01  -2.84056447e-01  -9.77153625e-01  -6.71236327e-02
  -2.03895831e-01  -4.92696079e-02  -2.80220143e-01  -3.18509589e-01
  -1.90206455e-01  -8.53376200e-01   3.54135607e-02  -2.95652986e-01
   1.57572686e-02  -3.28933437e-01  -1.59278605e-01  -6.33230843e-01
  -1.86892825e-01  -9.05868930e-02  -4.11336684e-02  -1.15366588e-01
  -3.29768284e-02  -1.67770897e-01  -2.18100233e-01  -1.92024440e-01
  -5.91401423e-01   4.26489595e-02  -1.53339472e-01   1.48781180e-02
  -1.29818756e-01   1.45513951e-02  -1.85151104e-01  -1.10302809e-01
  -4.09184876e-01  -2.10798012e-01  -3.83781435e-02  -3.56563679e-02
  -4.52388677e-02  -1.79052300e-02  -6.42803048e-02  -1.26147039e-02
  -1.03059025e-01  -1.50823023e-01  -1.58987034e-01  -4.21700386e-01
   3.56090129e-02  -8.81273236e-02   1.00993005e-02  -5.70046727e-02
   6.77354787e-03  -6.74706153e-02   1.29992453e-02  -1.11051973e-01
  -8.49978342e-02  -2.68540418e-01  -1.97483190e-01  -1.53100034e-02
  -2.86980653e-02  -1.82102569e-02  -1.24613779e-02  -2.38394851e-02
  -6.06708518e-03  -3.82505586e-02  -1.83802437e-03  -6.67938687e-02
  -1.08829077e-01  -1.24659979e-01  -3.07994299e-01   2.64518548e-02
  -5.43497372e-02   6.98245550e-03  -2.83201915e-02   2.70787618e-03
  -2.63466178e-02   4.77447913e-03  -3.91407704e-02   1.15527187e-02
  -7.03368547e-02  -6.95210270e-02  -1.80344325e-01  -1.72048911e-01
  -5.47521748e-03  -2.22987308e-02  -7.42943729e-03  -8.63887952e-03
  -9.40591713e-03  -4.43180361e-03  -1.34148622e-02  -9.67176972e-04
  -2.47119423e-02   3.29403161e-03  -4.56210773e-02  -8.24677278e-02
  -9.65024056e-02  -2.29738561e-01   1.85258430e-02  -3.51384800e-02
   4.72018650e-03  -1.55240497e-02   1.39740046e-03  -1.16594849e-02
   1.19124992e-03  -1.39390152e-02   4.04370544e-03  -2.48939966e-02
   1.02538005e-02  -4.68081953e-02  -5.88899271e-02  -1.24749463e-01
  -1.45018258e-01  -1.57983611e-03  -1.70054838e-02  -3.00853930e-03
  -6.03454679e-03  -3.92911834e-03  -2.97920489e-03  -5.09736970e-03
  -1.44167844e-03  -8.17883038e-03   1.19043015e-03  -1.72968829e-02
   5.57315818e-03  -3.25819703e-02  -6.53895286e-02  -7.54226296e-02
  -1.74675103e-01   1.24708298e-02  -2.34829663e-02   3.09012293e-03
  -9.13436500e-03   8.32318133e-04  -5.79083688e-03   3.02843716e-04
  -5.56306821e-03   8.61010725e-04  -8.24973064e-03   3.59331400e-03
  -1.71670381e-02   9.16688590e-03  -3.25860722e-02  -5.09721099e-02
  -8.97362022e-02  -1.20459959e-01  -2.89275954e-04  -1.28352587e-02
  -1.19226730e-03  -4.25477335e-03  -1.70859972e-03  -1.97800444e-03
  -2.11997893e-03  -1.08853262e-03  -2.92730568e-03  -2.59745631e-04
  -5.44639861e-03   2.07485981e-03  -1.29560727e-02   6.45691527e-03
  -2.41585899e-02  -5.38016666e-02  -6.06616032e-02  -1.35177857e-01
   8.09303963e-03  -1.60925202e-02   1.96166533e-03  -5.65049524e-03
   5.08251909e-04  -3.14658375e-03   8.74458163e-05  -2.51640732e-03
   1.13335145e-04  -2.99446682e-03   7.59845640e-04  -5.39858507e-03
   3.24857528e-03  -1.26821829e-02   8.28121984e-03  -2.36434947e-02
  -4.46954927e-02  -6.80711101e-02]
</code></pre>

<p>We&rsquo;ve found a set of theta that allows us to get the cost down to ~0.52 from the ~0.69 we saw above in our initial thetas of zero.</p>

<p>Let&rsquo;s reuse one of our plotting methods earlier (in this case seaborn) in this notebook and draw the decision boundry over it. Thanks to <a href="https://github.com/Nonnormalizable/NgMachineLearningPython/blob/master/ex2/helperFunctions.py">github user Nonnormalizable</a> for the boundry code.</p>

<pre><code class="language-python">def plot_decision_boundary(theta, X, y):
    # Simple seaborn scatterplot
    sns.lmplot('x1','x2',hue='y',data=data,fit_reg=False,size = 6,aspect= 1.5,palette='Dark2',legend=False)
    
    z = np.zeros([50, 50])
    uu = np.linspace(-1.0, 1.5, 50)
    vv = np.linspace(-1.0, 1.5, 50)
    for i, u in enumerate(uu):
        for j, v in enumerate(vv):
            z[i, j] = np.dot(new_features(np.array([[u]]), np.array([[v]])), theta)[0]
    plt.contour(uu, vv, z.T, [0], colors='dodgerblue')
    plt.axis([-.8, 1.2, -.8, 1.2])
    plt.xticks(np.arange(-.8, 1.3, .2))
    plt.yticks(np.arange(-.8, 1.3, .2))
    plt.xlabel('Microchip Test 1')
    plt.ylabel('Microchip Test 2')
    plt.legend(('y=1', 'y=0'), loc='upper right', numpoints=1)
    plt.title('Decision boundary for lambda = ' + str(lambda2), fontsize=13)
    # plt.savefig('temp.png', transparent=True,orientation='landscape', pad_inches=0.4)

plot_decision_boundary(optimal_theta, X, y)
</code></pre>

<p><img src="output_34_0.png" alt="png" /></p>

<p>Let&rsquo;s compute the accuracy on the training set:</p>

<pre><code class="language-python">def predict(theta, X):
    h_theta = h_of_theta(theta, X)
    return np.round(h_theta)
p = predict(optimal_theta, X)
print '\nTraining Accuracy: ', np.mean((p == y) * 100)
</code></pre>

<p><code>Training Accuracy:  83.0508474576</code></p>

<p>Let&rsquo;s see what happens with a smaller lambda value:</p>

<pre><code class="language-python">initial_theta = np.zeros(n)
lambda2 = .0001
myargs = (X, y, lambda2)
opts = {'full_output': True, 'maxiter': 400}

optimal_theta2, cost, grad_at_min, inv_hessian_matrix,\
fun_calls, grad_calls, warn_flags = optimize.fmin_bfgs(cost_function_reg,
                                initial_theta,
                                args=myargs,
                                fprime=gradient,
                                **opts)

print '\nCost at theta found by fmin_bfgs:', cost

plot_decision_boundary(optimal_theta2, X, y)
</code></pre>

<pre><code>Optimization terminated successfully.
         Current function value: 0.263489
         Iterations: 341
         Function evaluations: 342
         Gradient evaluations: 342

Cost at theta found by fmin_bfgs: 0.263489151662
</code></pre>

<p><img src="output_38_1.png" alt="png" /></p>

<p>Now let&rsquo;s look at what happens with a larger value for lambda:</p>

<pre><code class="language-python">initial_theta = np.zeros(n)
lambda2 = 10
myargs = (X, y, lambda2)
opts = {'full_output': True, 'maxiter': 400}

optimal_theta3, cost, grad_at_min, inv_hessian_matrix,\
fun_calls, grad_calls, warn_flags = optimize.fmin_bfgs(cost_function_reg,
                                initial_theta,
                                args=myargs,
                                fprime=gradient,
                                **opts)

print '\nCost at theta found by fmin_bfgs:', cost

plot_decision_boundary(optimal_theta3, X, y)
</code></pre>

<pre><code>Optimization terminated successfully.
         Current function value: 0.630696
         Iterations: 21
         Function evaluations: 22
         Gradient evaluations: 22

Cost at theta found by fmin_bfgs: 0.630695565717
</code></pre>

<p><img src="output_40_1.png" alt="png" /></p>

<p><img src="http://kldavenport.com/scikit-learn-logo.png" alt="sci-kit learn logo" /></p>

<h2 id="regularized-logistic-regression-with-scikit-learn">Regularized Logistic Regression with scikit-learn</h2>

<p>What we did above was more verbose than necessary, but could have been much worse without NumPy. Below we&rsquo;ll work on the same problem from beginning to end with substantially less lines of code.</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from scipy import optimize
import pandas as pd
import seaborn as sns
from ggplot import *
</code></pre>

<h2 id="loading-data">Loading data</h2>

<pre><code class="language-python">data = pd.read_csv('ex2data2.txt',header=None, names = ('x1','x2','y'))
data.info()
# Get the df values into numpy arrays
X = data.ix[:,0:2].values
y = data.ix[:,2].values
y = y # Note we left it as a standard 1d array this time

# No need to manually create the first zero coefficient
# X = np.hstack((np.zeros(shape=(X.shape[0],1)),X))
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 118 entries, 0 to 117
Data columns (total 3 columns):
x1    118 non-null float64
x2    118 non-null float64
y     118 non-null int64
dtypes: float64(2), int64(1)
memory usage: 3.7 KB
</code></pre>

<h2 id="fitting-and-scoring-the-regularized-logistic-regression-model">Fitting and scoring the Regularized Logistic Regression model</h2>

<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import train_test_split
from sklearn import metrics
from sklearn.cross_validation import cross_val_score

model = LogisticRegression(tol = .00000000000000000000001)
model = model.fit(X, y)

# check the accuracy on the training set
model.score(X, y)
</code></pre>

<p><code>0.5423728813559322</code></p>

<p>54% accuracy is pretty poor performance, let&rsquo;s check out what percentage of chips were labeled &ldquo;1&rdquo; (passed the test) below:</p>

<pre><code class="language-python">y.mean()
</code></pre>

<p><code>0.49152542372881358</code></p>

<p>49% means that we could have acheived similar performance just by guessing all ones or zeros for our label set. Hold on though, we forgot our feature engineering step. Let&rsquo;s use the <code>new_features</code> function we defined above to create many new features in X that might capture the non-linear relationship between variables x1 and x1.</p>

<pre><code class="language-python">print 'X shape before:', X.shape
X = new_features(X[:, 0][:, None], X[:, 1][:, None])
print 'X shape after:', X.shape
</code></pre>

<p><code>X shape before: (118, 2)</code>
<code>X shape after: (118, 190)</code></p>

<p>Now let&rsquo;s try fitting and scoring the model again:</p>

<pre><code class="language-python">model = LogisticRegression(tol = .00000000000000000000001)
model = model.fit(X, y)
model.score(X, y)
</code></pre>

<p><code>0.83050847457627119</code></p>

<p>83% is a marked improvement. Now we can move forward with more standard ML scoring methods. Up to this point we&rsquo;ve been using the same set of data to train and score our data. Let&rsquo;s create a train and test set using scikit&rsquo;s handy <code>train_test_split</code> function.</p>

<p><strong>Cross Validation Approach</strong>
1. Use the training set
2. Split it into training/test sets
3. Build a model on the train set
4. Evaluate on the test set
5. Repeat and average the estimated errors</p>

<p><strong>Used to</strong>
1. Choose variables to include in a model
2. Choose the type of prediction function to use
3. Choose the parameters in the prediction function
4. Compare different predictors</p>

<pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
model2 = LogisticRegression(tol = .00000000000000000000001 )
model2.fit(X_train, y_train)
</code></pre>

<p><code>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, penalty='l2', random_state=None, tol=1e-23)</code></p>

<pre><code class="language-python">predicted = model2.predict(X_test)
print metrics.accuracy_score(y_test, predicted)
print metrics.roc_auc_score(y_test, predicted)
</code></pre>

<p><code>0.861111111111</code><br />
<code>0.865325077399</code></p>

<p>We can even try a 10-fold cross-validation to observe how the accuracy changes.</p>

<h4 id="k-fold-cv-considerations">K-fold CV considerations</h4>

<ul>
<li>Larger k = less bias and more variance</li>
<li>Smaller k = more bias, less variance</li>
</ul>

<pre><code class="language-python">scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)
print scores
print scores.mean()
</code></pre>

<p><code>[ 0.66666667  0.75        0.91666667  0.25        0.66666667  0.66666667
      1.          0.66666667  0.72727273  0.81818182] 0.712878787879</code></p>

<p>We end up with an average score of 71%. Accuracy doesn&rsquo;t mean the same thing in all cases. Depending on your ML use case certain accuracy metrics might be more important that others. True negatives = Specificity and True Positives = Sensitivity. Much more on this: <a href="http://en.wikipedia.org/wiki/Precision_and_recall">here</a></p>

<p>We don&rsquo;t go over more tuning in this post, but there are many opportunities to use scikit&rsquo;s built in grid and random search features to find an optimal tolerance level instead of our arbitrary <code>tol = .00000000000000000000001</code>. A quick google search will reveal that there is an incredible amount of work being done just in hyperparameter estimation.</p>

<pre><code class="language-python">print metrics.confusion_matrix(y_test, predicted)
</code></pre>

<p><code>[[15  4]</code><br />
<code>[ 1 16]]</code></p>

<pre><code class="language-python">print metrics.classification_report(y_test, predicted)
</code></pre>

<pre><code>             precision    recall  f1-score   support

          0       0.94      0.79      0.86        19
          1       0.80      0.94      0.86        17

avg / total       0.87      0.86      0.86        36
</code></pre>

  </main>

          <footer role="contentinfo">
  <div>
    <label for="themer">
      dark theme: <input type="checkbox" id="themer" class="vh">
      <span aria-hidden="true"></span>
    </label>
  </div>
  
    Made with <a href="https://gohugo.io/">Hugo</a>. Deployed to <a href="https://app.netlify.com/">Netlify</a>. Source on <a href="https://github.com/kevindavenport/kldavenport.com">Github</a>.
  
</footer>

        </div>
      </div>
    </div>
    <script src="https://kldavenport.netlify.com/js/prism.js"></script>
<script src="https://kldavenport.netlify.com/js/dom-scripts.js"></script>

    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-34706513-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>
