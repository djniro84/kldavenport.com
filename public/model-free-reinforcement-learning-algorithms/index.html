<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.54.0" />
  <link rel="canonical" href="https://kldavenport.com/model-free-reinforcement-learning-algorithms/">

  

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://kldavenport.com/css/prism.css" media="none" onload="this.media='all';">

  
  
  <link rel="stylesheet" type="text/css" href="https://kldavenport.com/css/styles.css">

  <style id="inverter" media="none">
    html { filter: invert(100%) }
    * { background-color: inherit }
    img:not([src*=".svg"]), .colors, iframe, .demo-container { filter: invert(100%) }
  </style>

  
  
  <title>Model Free Reinforcement Learning Algorithms | Kevin Davenport Engineering &amp; ML blog</title>
</head>

  <body>
    <a href="#main">skip to content</a>
    <svg style="display: none">
  <symbol id="bookmark" viewBox="0 0 40 50">
   <g transform="translate(2266 3206.2)">
    <path style="stroke:currentColor;stroke-width:3.2637;fill:none" d="m-2262.2-3203.4-.2331 42.195 16.319-16.318 16.318 16.318.2331-42.428z"/>
   </g>
  </symbol>

  <symbol id="w3c" viewBox="0 0 127.09899 67.763">
   <text font-size="83" style="font-size:83px;font-family:Trebuchet;letter-spacing:-12;fill-opacity:0" letter-spacing="-12" y="67.609352" x="-26.782778">W3C</text>
   <text font-size="83" style="font-size:83px;font-weight:bold;font-family:Trebuchet;fill-opacity:0" y="67.609352" x="153.21722" font-weight="bold">SVG</text>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m33.695.377 12.062 41.016 12.067-41.016h8.731l-19.968 67.386h-.831l-12.48-41.759-12.479 41.759h-.832l-19.965-67.386h8.736l12.061 41.016 8.154-27.618-3.993-13.397h8.737z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m91.355 46.132c0 6.104-1.624 11.234-4.862 15.394-3.248 4.158-7.45 6.237-12.607 6.237-3.882 0-7.263-1.238-10.148-3.702-2.885-2.47-5.02-5.812-6.406-10.022l6.82-2.829c1.001 2.552 2.317 4.562 3.953 6.028 1.636 1.469 3.56 2.207 5.781 2.207 2.329 0 4.3-1.306 5.909-3.911 1.609-2.606 2.411-5.738 2.411-9.401 0-4.049-.861-7.179-2.582-9.399-1.995-2.604-5.129-3.912-9.397-3.912h-3.327v-3.991l11.646-20.133h-14.062l-3.911 6.655h-2.493v-14.976h32.441v4.075l-12.31 21.217c4.324 1.385 7.596 3.911 9.815 7.571 2.22 3.659 3.329 7.953 3.329 12.892z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.21 0 1.414 8.6-5.008 9.583s-1.924-4.064-5.117-6.314c-2.693-1.899-4.447-2.309-7.186-1.746-3.527.73-7.516 4.938-9.258 10.13-2.084 6.21-2.104 9.218-2.178 11.978-.115 4.428.58 7.043.58 7.043s-3.04-5.626-3.011-13.866c.018-5.882.947-11.218 3.666-16.479 2.404-4.627 5.954-7.404 9.114-7.728 3.264-.343 5.848 1.229 7.841 2.938 2.089 1.788 4.213 5.698 4.213 5.698l4.94-9.837z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.82 48.674s-2.208 3.957-3.589 5.48c-1.379 1.524-3.849 4.209-6.896 5.555-3.049 1.343-4.646 1.598-7.661 1.306-3.01-.29-5.807-2.032-6.786-2.764-.979-.722-3.486-2.864-4.897-4.854-1.42-2-3.634-5.995-3.634-5.995s1.233 4.001 2.007 5.699c.442.977 1.81 3.965 3.749 6.572 1.805 2.425 5.315 6.604 10.652 7.545 5.336.945 9.002-1.449 9.907-2.031.907-.578 2.819-2.178 4.032-3.475 1.264-1.351 2.459-3.079 3.116-4.108.487-.758 1.276-2.286 1.276-2.286l-1.276-6.644z"/>
  </symbol>

  <symbol id="tag" viewBox="0 0 177.16535 177.16535">
    <g transform="translate(0 -875.2)">
     <path style="fill-rule:evenodd;stroke-width:0;fill:currentColor" d="m159.9 894.3-68.79 8.5872-75.42 77.336 61.931 60.397 75.429-76.565 6.8495-69.755zm-31.412 31.835a10.813 10.813 0 0 1 1.8443 2.247 10.813 10.813 0 0 1 -3.5174 14.872l-.0445.0275a10.813 10.813 0 0 1 -14.86 -3.5714 10.813 10.813 0 0 1 3.5563 -14.863 10.813 10.813 0 0 1 13.022 1.2884z"/>
    </g>
  </symbol>

  <symbol id="balloon" viewBox="0 0 141.73228 177.16535">
   <g transform="translate(0 -875.2)">
    <g>
     <path style="fill:currentColor" d="m68.156 882.83-.88753 1.4269c-4.9564 7.9666-6.3764 17.321-5.6731 37.378.36584 10.437 1.1246 23.51 1.6874 29.062.38895 3.8372 3.8278 32.454 4.6105 38.459 4.6694-.24176 9.2946.2879 14.377 1.481 1.2359-3.2937 5.2496-13.088 8.886-21.623 6.249-14.668 8.4128-21.264 10.253-31.252 1.2464-6.7626 1.6341-12.156 1.4204-19.764-.36325-12.93-2.1234-19.487-6.9377-25.843-2.0833-2.7507-6.9865-7.6112-7.9127-7.8436-.79716-.20019-6.6946-1.0922-6.7755-1.0248-.02213.0182-5.0006-.41858-7.5248-.22808l-2.149-.22808h-3.3738z"/>
     <path style="fill:currentColor" d="m61.915 883.28-3.2484.4497c-1.7863.24724-3.5182.53481-3.8494.63994-2.4751.33811-4.7267.86957-6.7777 1.5696-.28598 0-1.0254.20146-2.3695.58589-5.0418 1.4418-6.6374 2.2604-8.2567 4.2364-6.281 7.6657-11.457 18.43-12.932 26.891-1.4667 8.4111.71353 22.583 5.0764 32.996 3.8064 9.0852 13.569 25.149 22.801 37.517 1.3741 1.841 2.1708 2.9286 2.4712 3.5792 3.5437-1.1699 6.8496-1.9336 10.082-2.3263-1.3569-5.7831-4.6968-21.86-6.8361-33.002-.92884-4.8368-2.4692-14.322-3.2452-19.991-.68557-5.0083-.77707-6.9534-.74159-15.791.04316-10.803.41822-16.162 1.5026-21.503 1.4593-5.9026 3.3494-11.077 6.3247-15.852z"/>
     <path style="fill:currentColor" d="m94.499 885.78c-.10214-.0109-.13691 0-.0907.0409.16033.13489 1.329 1.0675 2.5976 2.0723 6.7003 5.307 11.273 14.568 12.658 25.638.52519 4.1949.24765 14.361-.5059 18.523-2.4775 13.684-9.7807 32.345-20.944 53.519l-3.0559 5.7971c2.8082.76579 5.7915 1.727 8.9926 2.8441 11.562-11.691 18.349-19.678 24.129-28.394 7.8992-11.913 11.132-20.234 12.24-31.518.98442-10.02-1.5579-20.876-6.7799-28.959-.2758-.4269-.57803-.86856-.89617-1.3166-3.247-6.13-9.752-12.053-21.264-16.131-2.3687-.86369-6.3657-2.0433-7.0802-2.1166z"/>
     <path style="fill:currentColor" d="m32.52 892.22c-.20090-.13016-1.4606.81389-3.9132 2.7457-11.486 9.0476-17.632 24.186-16.078 39.61.79699 7.9138 2.4066 13.505 5.9184 20.562 5.8577 11.77 14.749 23.219 30.087 38.74.05838.059.12188.1244.18052.1838 1.3166-.5556 2.5965-1.0618 3.8429-1.5199-.66408-.32448-1.4608-1.3297-3.8116-4.4602-5.0951-6.785-8.7512-11.962-13.051-18.486-5.1379-7.7948-5.0097-7.5894-8.0586-13.054-6.2097-11.13-8.2674-17.725-8.6014-27.563-.21552-6.3494.13041-9.2733 1.775-14.987 2.1832-7.5849 3.9273-10.986 9.2693-18.07 1.7839-2.3656 2.6418-3.57 2.4409-3.7003z"/>
     <path style="fill:currentColor" d="m69.133 992.37c-6.2405.0309-12.635.76718-19.554 2.5706 4.6956 4.7759 9.935 10.258 12.05 12.625l4.1272 4.6202h11.493l3.964-4.4516c2.0962-2.3541 7.4804-7.9845 12.201-12.768-8.378-1.4975-16.207-2.6353-24.281-2.5955z"/>
     <rect style="stroke-width:0;fill:currentColor" ry="2.0328" height="27.746" width="22.766" y="1017.7" x="60.201"/>
    </g>
   </g>
  </symbol>

  <symbol id="info" viewBox="0 0 41.667 41.667">
   <g transform="translate(-37.035 -1004.6)">
    <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m76.25 1030.2a18.968 18.968 0 0 1 -23.037 13.709 18.968 18.968 0 0 1 -13.738 -23.019 18.968 18.968 0 0 1 23.001 -13.768 18.968 18.968 0 0 1 13.798 22.984"/>
    <g transform="matrix(1.1146 0 0 1.1146 -26.276 -124.92)">
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m75.491 1039.5v-8.7472"/>
     <path style="stroke-width:0;fill:currentColor" transform="scale(-1)" d="m-73.193-1024.5a2.3719 2.3719 0 0 1 -2.8807 1.7142 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
   </g>
  </symbol>

  <symbol id="warning" viewBox="0 0 48.430474 41.646302">
    <g transform="translate(-1.1273 -1010.2)">
     <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:4.151;fill:none" d="m25.343 1012.3-22.14 37.496h44.28z"/>
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:4.1512;fill:none" d="m25.54 1027.7v8.7472"/>
     <path style="stroke-width:0;fill:currentColor" d="m27.839 1042.8a2.3719 2.3719 0 0 1 -2.8807 1.7143 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
  </symbol>

  <symbol id="menu" viewBox="0 0 50 50">
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="0" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="20" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="40" x="0"/>
   </symbol>

   <symbol id="link" viewBox="0 0 50 50">
    <g transform="translate(0 -1002.4)">
     <g transform="matrix(.095670 0 0 .095670 2.3233 1004.9)">
      <g>
       <path style="stroke-width:0;fill:currentColor" d="m452.84 192.9-128.65 128.65c-35.535 35.54-93.108 35.54-128.65 0l-42.881-42.886 42.881-42.876 42.884 42.876c11.845 11.822 31.064 11.846 42.886 0l128.64-128.64c11.816-11.831 11.816-31.066 0-42.9l-42.881-42.881c-11.822-11.814-31.064-11.814-42.887 0l-45.928 45.936c-21.292-12.531-45.491-17.905-69.449-16.291l72.501-72.526c35.535-35.521 93.136-35.521 128.64 0l42.886 42.881c35.535 35.523 35.535 93.141-.001 128.66zm-254.28 168.51-45.903 45.9c-11.845 11.846-31.064 11.817-42.881 0l-42.884-42.881c-11.845-11.821-11.845-31.041 0-42.886l128.65-128.65c11.819-11.814 31.069-11.814 42.884 0l42.886 42.886 42.876-42.886-42.876-42.881c-35.54-35.521-93.113-35.521-128.65 0l-128.65 128.64c-35.538 35.545-35.538 93.146 0 128.65l42.883 42.882c35.51 35.54 93.11 35.54 128.65 0l72.496-72.499c-23.956 1.597-48.092-3.784-69.474-16.283z"/>
      </g>
     </g>
    </g>
  </symbol>

  <symbol id="doc" viewBox="0 0 35 45">
   <g transform="translate(-147.53 -539.83)">
    <path style="stroke:currentColor;stroke-width:2.4501;fill:none" d="m149.38 542.67v39.194h31.354v-39.194z"/>
    <g style="stroke-width:25" transform="matrix(.098003 0 0 .098003 133.69 525.96)">
     <path d="m220 252.36h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path style="stroke:currentColor;stroke-width:25;fill:none" d="m220 409.95h200"/>
     <path d="m220 488.74h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path d="m220 331.15h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
    </g>
   </g>
 </symbol>

 <symbol id="tick" viewBox="0 0 177.16535 177.16535">
  <g transform="translate(0 -875.2)">
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="155" width="40" y="702.99" x="556.82"/>
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="40" width="90.404" y="817.99" x="506.42"/>
  </g>
 </symbol>
</svg>

    <div class="wrapper">
      <header class="intro-and-nav" role="banner">
  <div>
    <div class="intro">
      <a class="logo" href="/" aria-label="Kevin Davenport Engineering &amp; ML blog home page">
        <img src="https://kldavenport.com/images/logo.svg" alt="">
      </a>
      <p class="library-desc">
        
      </p>
    </div>
    <nav id="patterns-nav" class="patterns" role="navigation">
  <h2 class="vh">Main navigation</h2>
  <button id="menu-button" aria-expanded="false">
    <svg viewBox="0 0 50 50" aria-hidden="true" focusable="false">
      <use xlink:href="#menu"></use>
    </svg>
    Menu
  </button>
  
  <ul id="patterns-list">
  
    <li class="pattern">
      
      
      
      
      <a href="/post/" aria-current="page">
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Blog</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/about/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">About</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/bike-travel/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Bike Travel</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/tags/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Tags</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/index.xml" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">RSS</span>
      </a>
    </li>
  
  </ul>
</nav>
  </div>
</header>
      <div class="main-and-footer">
        <div>
          
  <main id="main">
    <h1>
      <svg class="bookmark-icon" aria-hidden="true" viewBox="0 0 40 50" focusable="false">
        <use xlink:href="#bookmark"></use>
      </svg>
      Model Free Reinforcement Learning Algorithms
    </h1>
    <div class="date">
      
      <strong aria-hidden="true">Publish date: </strong>Jan 21, 2014
    </div>
    
      <div class="tags">
        <strong aria-hidden="true">Tags: </strong>
        <ul aria-label="tags">
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.com/tags/reinforcement-learning">reinforcement learning</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.com/tags/sutton">sutton</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.com/tags/monte-carlo">monte carlo</a>
            </li>
          
        </ul>
      </div>
    
    
    
      

  <nav class="toc" aria-labelledby="toc-heading">
    <h2 id="toc-heading">Table of contents</h2>
    <ol>
      
        <li>
          <a href="#td-learning">
            TD-Learning
          </a>
        </li>
      
        <li>
          <a href="#setting-up-our-environment">
            Setting up our environment
          </a>
        </li>
      
        <li>
          <a href="#utility-functions">
            Utility Functions
          </a>
        </li>
      
        <li>
          <a href="#figure-3">
            Figure 3
          </a>
        </li>
      
        <li>
          <a href="#figure-5">
            Figure 5
          </a>
        </li>
      
    </ol>
  </nav>


    
    

<p>Reproducibility means different things to many people working the applied sciences space. The continuum appears to be sharing:
1. Code (.py, .cpp files)
2. Jupyter Notebook
3. Docker Container</p>

<p>With #1 and #2 you are putting the onus on the recepient to have the same environment as you, maybe you point them to the same Anaconda distribution. With #3 you get an easy way to share your working environments including libraries and drivers. On a related note please check out this entertaining and education talk on <a href="https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit">&ldquo;I Don&rsquo;t Like Notebooks&rdquo;</a> by Joel Grus.</p>

<p>A critical aspect of research is the reproduction of previously published results. Yet most will find reproduction of research challenging since important parameters needed to reproduce results are often not stated in the papers. I’ve noticed in the past 5 years there has been a sort of catharsis regarding the lack of reproducibility <a href="https://www.nature.com/collections/prbfkwmwvz/">[1]</a><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5220388/">[2]</a><a href="https://www.sciencemag.org/news/2016/02/if-you-fail-reproduce-another-scientist-s-results-journal-wants-know?r3f_986=http://kldavenport.com/suttons-temporal-difference-learning/">[3]</a>. This isn’t an issue for wetlab science alone <a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002165">[4]</a>. The obvious benefit of reproduction is to aid in your own understanding of the results. This then enables one to extend and compare new contributions to existing publications.</p>

<p>I struggled with the intuition behind Sutton’s <a href="https://www.semanticscholar.org/paper/Learning-to-predict-by-the-methods-of-temporal-Sutton/094ca99cc94e38984823776158da738e5bc3963d">Learning to Predict by the Methods of Temporal Differences paper</a>. I hit the wall early on the “A Random-Walk” example (Page 19 3.2). I read Chapter 6 and 12 of Sutton’s Reinforcement Learning textbook to gain more intuition. <a href="http://incompleteideas.net/book/the-book.html">(available for free)</a>. To work through my reproduction of the &ldquo;A Random Walk&rdquo; below, I recommend at minimum the reader has a basic understanding of <a href="https://en.wikipedia.org/wiki/Reinforcement_learning#Value_function">Value-Functions</a> and <a href="https://en.wikipedia.org/wiki/Q-learning">Q-Learning</a>.</p>

<h2 id="td-learning">TD-Learning</h2>

<p>Temporal-Difference (TD) algorithms work without an explicit model and learn from the experience and outcomes of iterating over multiple episodes (or sequences). TD Learning is similar to Monte Carlo methods, however TD can learn from individual steps without needing the final result. These are among other differences:</p>

<table width="950">
<tbody>
<tr>
<td style="text-align: left;" width="355"><strong>Monte Carlo methods</strong></td>
<td style="text-align: left;" width="355"><strong>TD learning</strong></td>
</tr>
<tr>
<td style="text-align: left;">MC must wait until the end of the episode before the return is known.</td>
<td style="text-align: left;">TD can learn online after every step and does not need to wait until the end of episode.</td>
</tr>
<tr>
<td style="text-align: left;">MC has high variance and low bias.</td>
<td style="text-align: left;">TD has low variance and some decent bias.</td>
</tr>
<tr>
<td style="text-align: left;">MC does not exploit the Markov property.</td>
<td style="text-align: left;">TD exploits the Markov property.</td>
</tr>
</tbody>
</table>

<p>Rather than computing the estimate of a next state, TD can estimate n-steps into future. In the case of TD-λ , we use lambda to set the myopicness of our reward emphasis.  The value of λ can be optimized to for a performance/speed tradeoff. The λ parameter is also called the trace decay parameter, with 0 ≤ λ ≤ 1. The higher the value, the longer lasting the traces. In this case, a larger proportion of credit from a reward can be given to more distant states and actions. λ = 1 is essentially Monte Carlo.</p>










  


<figure role="group" aria-describedby="caption-d41d8cd98f00b204e9800998ecf8427e">
  <a href="https://kldavenport.com/model-free-reinforcement-learning-algorithms/image1.png" class="img-link">
    <img src="https://kldavenport.com/model-free-reinforcement-learning-algorithms/image1_hu12ec4ae2e534ed3d629d412ca1bb68b7_49613_700x0_resize_box_2.png">
  </a>
  <figcaption id="caption-d41d8cd98f00b204e9800998ecf8427e">
    
  </figcaption>
</figure>


<p>Figure 2 from the book. A generator of bounded random walks. This Markov process generated the data sequences in the example. All walks begin in state D. From states B, C, D, E, and F, the walk has a 50% chance of moving either to the right or to the left. If either edge state, A or G, is entered, then the walk terminates.</p>

<h2 id="setting-up-our-environment">Setting up our environment</h2>

<pre><code class="language-python">import math, sys, json, random 
import numpy as np
import pandas as pd
import matplotlib 
import matplotlib.style
import matplotlib.pyplot as plt
# import seaborn as sns
from sklearn.externals import joblib # Still better than pickle in 2018?
# To protect from IPython kernel switching mistakes
from __future__ import division
%matplotlib inline
</code></pre>

<h2 id="utility-functions">Utility Functions</h2>

<p>We implement the TD Lambda algorithm below as <code>tdlEstimate</code> the image below is from chapter 6 of the Sutton textbook.</p>










  


<figure role="group" aria-describedby="caption-d41d8cd98f00b204e9800998ecf8427e">
  <a href="https://kldavenport.com/model-free-reinforcement-learning-algorithms/image2.png" class="img-link">
    <img src="https://kldavenport.com/model-free-reinforcement-learning-algorithms/image2_hu835ee27a48a01f9a3eb035d4827c3a38_128186_700x0_resize_box_2.png">
  </a>
  <figcaption id="caption-d41d8cd98f00b204e9800998ecf8427e">
    
  </figcaption>
</figure>


<pre><code class="language-python"># TD Lambda
def tdlEstimate(alpha, _lambda, state_sequence, values):
    &quot;&quot;&quot;
    alphas: array of arbitrary values (e.g. 0.005, 0.01, 0.015)
    _lambda: chosen from an arbitrary array (e.g. 0.1, 1)
    state sequence: an array chosen from an arbitrary set of sequence simulations such as [3, 4, 5, 6] or \
    [3, 4, 3, 2, 3, 4, 3, 4, 5, 6] per the MDP figure 2 above.
    returns: &quot;&quot;&quot;
    
    # Per figure 2, we have 7 possible states, with two of them being end states (A,G)
    
    eligibility = np.zeros(7)
    updates     = np.zeros(7)

    for t in range(0, len(state_sequence) - 1):
        current_state = state_sequence[t]
        next_state = state_sequence[t+1]

        eligibility[current_state] += 1.0

        td = alpha * (values[next_state] - values[current_state])

        updates += td * eligibility
        eligibility *= _lambda

    return updates

# Simulator to generate random walk sequences in our MDP defined in fig 2 above
states = ['A', 'B', 'C', 'D', 'E', 'F', 'G']

def simulate():
    &quot;&quot;&quot;returns: a sequence of states picked from a uniform random sample such as 
    [3, 2, 1, 0] or [3, 4, 3, 4, 3, 4, 5, 6]
    &quot;&quot;&quot;
    states = [3] # Start in center at &quot;D&quot;
    while states[-1] not in [0, 6]:
        states.append(states[-1] +  (1 if random.choice([True, False]) else -1)) # go left or right randomly

    return states


# Setup data for plots

random.seed(101)
# pg.20 gives true probabilities for states B, C, D, E, F
# truth = np.arange(1, 6) / 6.0
truth = [1 / 6, 1 / 3, 1 / 2, 2 / 3, 5 / 6]

dtype = np.float

num_train_sets = 100
num_sequences   = 10 # or episodes

training_sets = [[simulate() for i in range(num_sequences)] for i in range(num_train_sets)]
</code></pre>

<h2 id="figure-3">Figure 3</h2>

<pre><code class="language-python"># Figure 3
alphas  = np.array([0.005, 0.01, 0.015], dtype=dtype)
lambdas = np.array([0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0], dtype=dtype) # given in figure 3 caption

results = []

for _lambda in lambdas:
    for alpha in alphas:
        rmses = []
        for training_set in training_sets:
            # values initialized to zero and updates via tdlEstimate
            values = np.zeros(7, dtype=dtype)
            iterations = 0
            
            while True:
                iterations += 1
                before  = np.copy(values)
                updates = np.zeros(7, dtype=dtype)
                # The reward for reaching state &quot;G&quot; (element 7)
                values[6] = 1.0

                for sequence in training_set: 
                    updates += tdlEstimate(alpha, _lambda, sequence, values)

                values += updates
                diff = np.sum(np.absolute(before - values))

                if diff &lt; .000001:
                    break

            estimate = np.array(values[1:-1], dtype=dtype)
            error = (truth - estimate)
            rms   = np.sqrt(np.average(np.power(error, 2)))
            rmses.append(rms)

        result = [_lambda, alpha, np.mean(rmses), np.std(rmses)]
        results.append(result)

# outputs
# joblib.dump(results, 'results.pkl')

# results = joblib.load('results.pkl') 
data = pd.DataFrame(results)
data.columns = [&quot;lambda&quot;, &quot;alpha&quot;, &quot;rms&quot;, &quot;rmsstd&quot;]
data.head()
</code></pre>

<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th>lambda</th>
      <th>alpha</th>
      <th>rms</th>
      <th>rmsstd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.235702</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.05</td>
      <td>0.175010</td>
      <td>0.002556</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.10</td>
      <td>0.131192</td>
      <td>0.009537</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.15</td>
      <td>0.103448</td>
      <td>0.020459</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.20</td>
      <td>0.091540</td>
      <td>0.033838</td>
    </tr>
  </tbody>
</table>

<pre><code class="language-python">data = data[data.groupby(['lambda'])['rms'].transform(min) == data['rms']].set_index(keys=['lambda'])
data.drop('alpha', 1, inplace=True,)
data.drop('rmsstd', 1,inplace=True,)
</code></pre>

<p>I repeatedly calculated the TD equation on a given set until the RMSE between the weights (new values) and the ideal probabilities was less than my arbitrary threshold.
The values and trend are very similar to the original but more tweaking to my environment assumptions might have improved the similarity. Given more time and intellect I’d like to do a random search of a space of hyper-parameters and see what most closely approximates the original. RMS error increases with non-linearity in relation to lambda. The initial curtailing isn’t the same as Sutton with lambda 0 performing the best out right.</p>

<p>If we choose to imitate figure 3 literally we get the below. I would prefer grid lines though, which is just a matter of using default seaborn aesthetics.</p>

<pre><code class="language-python"># sns.set_style(&quot;white&quot;)
plt.figure(num=None, figsize=(10, 6), dpi=72)
plt.margins(.05)
plt.xlabel(r&quot;$\lambda$&quot;)
plt.ylabel(&quot;RMS&quot;)
plt.title(&quot;Figure 3&quot;)
plt.xticks([i * .1 for i in range(0, 10)])
plt.yticks([i * .01 for i in range(10, 19)])
plt.text(.79, .17, &quot;Widrow-Hoff&quot;, ha=&quot;center&quot;, va=&quot;center&quot;, rotation=0,size=15)
plt.text(-.22, .174, &quot;ERROR\nUSING\nBEST α&quot;,size=15)
plt.plot(data,marker='o');
</code></pre>










  


<figure role="group" aria-describedby="caption-d41d8cd98f00b204e9800998ecf8427e">
  <a href="https://kldavenport.com/model-free-reinforcement-learning-algorithms/image3.png" class="img-link">
    <img src="https://kldavenport.com/model-free-reinforcement-learning-algorithms/image3_huc9bb750ec3d5f807201264f9a59225f9_17724_700x0_resize_box_2.png">
  </a>
  <figcaption id="caption-d41d8cd98f00b204e9800998ecf8427e">
    
  </figcaption>
</figure>


<h2 id="figure-5">Figure 5</h2>

<pre><code class="language-python">%time 
alphas  = [0.05 * i for i in range(0,16)]
lambdas = [0.05 * i for i in range(0, 21)]

results = []

for _lambda in lambdas:
    for alpha in alphas:
        rms_vals = []
        for training_set in training_sets:

            values = np.array([0.5 for i in range(7)])

            for sequence in training_set:
                values[0] = 0.0
                values[6] = 1.0
                values += tdlEstimate(alpha, _lambda, sequence, values)

            estimate = np.array(values[1:-1])
            error = (truth - estimate)
            rms   = np.sqrt(np.average(np.power(error, 2)))

            rms_vals.append(rms)

        result = [_lambda, alpha, np.mean(rms_vals), np.std(rms_vals)]
        results.append(result)
</code></pre>

<pre><code class="language-bash">CPU times: user 3 µs, sys: 1 µs, total: 4 µs
Wall time: 5.96 µs
</code></pre>

<p>Here we reiterate that larger lambda values perform better via smaller learning rates. I believe this is due to the larger lambda values emphasizing weight on larger steps and the final output.</p>

<pre><code class="language-python">data = pd.DataFrame(results)

data.columns = [&quot;lambda&quot;, &quot;alpha&quot;, &quot;rms&quot;, &quot;rmsstd&quot;]

data = data[data.groupby(['lambda'])['rms'].transform(min) == \
            data['rms']].set_index(keys=['lambda'])

data = data.drop('alpha', 1)
data = data.drop('rmsstd', 1)

plt.figure(num=None, figsize=(10, 6), dpi=80)
plt.plot(data, marker='o') 
plt.margins(.10)
plt.xlabel(r&quot;$\lambda$&quot;)
plt.ylabel(&quot;RMS&quot;)
plt.title(&quot;Figure 5 &quot;)
plt.text(.7, .185, &quot;Widrow-Hoff&quot;, ha=&quot;center&quot;, va=&quot;center&quot;, rotation=0,size=15)
plt.text(-.25,.204, &quot;ERROR\nUSING\nBEST α&quot;,size=12)
</code></pre>










  


<figure role="group" aria-describedby="caption-d41d8cd98f00b204e9800998ecf8427e">
  <a href="https://kldavenport.com/model-free-reinforcement-learning-algorithms/image4.png" class="img-link">
    <img src="https://kldavenport.com/model-free-reinforcement-learning-algorithms/image4_huf6391ed5c86710d1ee6e07872c07519f_19592_700x0_resize_box_2.png">
  </a>
  <figcaption id="caption-d41d8cd98f00b204e9800998ecf8427e">
    
  </figcaption>
</figure>


<p>To conclude, <a href="https://www.youtube.com/embed/DZzffdHNqtQ">here is a short simple video</a> from Peter Norvig that provides good intuition on TD learning.</p>

  </main>

          <footer role="contentinfo">
  <div>
    <label for="themer">
      dark theme: <input type="checkbox" id="themer" class="vh">
      <span aria-hidden="true"></span>
    </label>
  </div>
  
    Made w/ <a href="https://gohugo.io/">Hugo</a>. Deployed by <a href="https://app.netlify.com/">Netlify</a>.
  
</footer>

        </div>
      </div>
    </div>
    <script src="https://kldavenport.com/js/prism.js"></script>
<script src="https://kldavenport.com/js/dom-scripts.js"></script>

    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-34706513-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>
