<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Entropy on Kevin Davenport Engineering &amp; ML blog</title>
    <link>https://kldavenport.com/tags/entropy/</link>
    <description>Recent content in Entropy on Kevin Davenport Engineering &amp; ML blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jun 2015 11:54:26 -0800</lastBuildDate>
    
	<atom:link href="https://kldavenport.com/tags/entropy/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Pure Python Decision Trees</title>
      <link>https://kldavenport.com/pure-python-decision-trees/</link>
      <pubDate>Wed, 17 Jun 2015 11:54:26 -0800</pubDate>
      
      <guid>https://kldavenport.com/pure-python-decision-trees/</guid>
      <description>By now we all know what Random Forests is. We know about the great off-the-self performance, ease of tuning and parallelization, as well as it’s importance measures. It’s easy for engineers implementing RF to forget about it’s underpinnings. Unlike some of it’s more modern and advanced contemporaries, descision trees are easy to interpret. A neural net might obtain great results but it is difficult to work backwards from and explain to stake holders as the weights of the connections between two neurons have little meaning on their own.</description>
    </item>
    
  </channel>
</rss>