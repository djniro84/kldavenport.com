<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.54.0" />
  <link rel="canonical" href="https://kldavenport.com/topic-modeling-amazon-reviews/">

  

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://kldavenport.com/css/prism.css" media="none" onload="this.media='all';">

  
  
  <link rel="stylesheet" type="text/css" href="https://kldavenport.com/css/styles.css">

  <style id="inverter" media="none">
    html { filter: invert(100%) }
    * { background-color: inherit }
    img:not([src*=".svg"]), .colors, iframe, .demo-container { filter: invert(100%) }
  </style>

  
  
  <title>Topic Modeling Amazon Product Reviews | Kevin Davenport Engineering &amp; ML blog</title>
</head>

  <body>
    <a href="#main">skip to content</a>
    <svg style="display: none">
  <symbol id="bookmark" viewBox="0 0 40 50">
   <g transform="translate(2266 3206.2)">
    <path style="stroke:currentColor;stroke-width:3.2637;fill:none" d="m-2262.2-3203.4-.2331 42.195 16.319-16.318 16.318 16.318.2331-42.428z"/>
   </g>
  </symbol>

  <symbol id="w3c" viewBox="0 0 127.09899 67.763">
   <text font-size="83" style="font-size:83px;font-family:Trebuchet;letter-spacing:-12;fill-opacity:0" letter-spacing="-12" y="67.609352" x="-26.782778">W3C</text>
   <text font-size="83" style="font-size:83px;font-weight:bold;font-family:Trebuchet;fill-opacity:0" y="67.609352" x="153.21722" font-weight="bold">SVG</text>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m33.695.377 12.062 41.016 12.067-41.016h8.731l-19.968 67.386h-.831l-12.48-41.759-12.479 41.759h-.832l-19.965-67.386h8.736l12.061 41.016 8.154-27.618-3.993-13.397h8.737z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m91.355 46.132c0 6.104-1.624 11.234-4.862 15.394-3.248 4.158-7.45 6.237-12.607 6.237-3.882 0-7.263-1.238-10.148-3.702-2.885-2.47-5.02-5.812-6.406-10.022l6.82-2.829c1.001 2.552 2.317 4.562 3.953 6.028 1.636 1.469 3.56 2.207 5.781 2.207 2.329 0 4.3-1.306 5.909-3.911 1.609-2.606 2.411-5.738 2.411-9.401 0-4.049-.861-7.179-2.582-9.399-1.995-2.604-5.129-3.912-9.397-3.912h-3.327v-3.991l11.646-20.133h-14.062l-3.911 6.655h-2.493v-14.976h32.441v4.075l-12.31 21.217c4.324 1.385 7.596 3.911 9.815 7.571 2.22 3.659 3.329 7.953 3.329 12.892z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.21 0 1.414 8.6-5.008 9.583s-1.924-4.064-5.117-6.314c-2.693-1.899-4.447-2.309-7.186-1.746-3.527.73-7.516 4.938-9.258 10.13-2.084 6.21-2.104 9.218-2.178 11.978-.115 4.428.58 7.043.58 7.043s-3.04-5.626-3.011-13.866c.018-5.882.947-11.218 3.666-16.479 2.404-4.627 5.954-7.404 9.114-7.728 3.264-.343 5.848 1.229 7.841 2.938 2.089 1.788 4.213 5.698 4.213 5.698l4.94-9.837z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.82 48.674s-2.208 3.957-3.589 5.48c-1.379 1.524-3.849 4.209-6.896 5.555-3.049 1.343-4.646 1.598-7.661 1.306-3.01-.29-5.807-2.032-6.786-2.764-.979-.722-3.486-2.864-4.897-4.854-1.42-2-3.634-5.995-3.634-5.995s1.233 4.001 2.007 5.699c.442.977 1.81 3.965 3.749 6.572 1.805 2.425 5.315 6.604 10.652 7.545 5.336.945 9.002-1.449 9.907-2.031.907-.578 2.819-2.178 4.032-3.475 1.264-1.351 2.459-3.079 3.116-4.108.487-.758 1.276-2.286 1.276-2.286l-1.276-6.644z"/>
  </symbol>

  <symbol id="tag" viewBox="0 0 177.16535 177.16535">
    <g transform="translate(0 -875.2)">
     <path style="fill-rule:evenodd;stroke-width:0;fill:currentColor" d="m159.9 894.3-68.79 8.5872-75.42 77.336 61.931 60.397 75.429-76.565 6.8495-69.755zm-31.412 31.835a10.813 10.813 0 0 1 1.8443 2.247 10.813 10.813 0 0 1 -3.5174 14.872l-.0445.0275a10.813 10.813 0 0 1 -14.86 -3.5714 10.813 10.813 0 0 1 3.5563 -14.863 10.813 10.813 0 0 1 13.022 1.2884z"/>
    </g>
  </symbol>

  <symbol id="balloon" viewBox="0 0 141.73228 177.16535">
   <g transform="translate(0 -875.2)">
    <g>
     <path style="fill:currentColor" d="m68.156 882.83-.88753 1.4269c-4.9564 7.9666-6.3764 17.321-5.6731 37.378.36584 10.437 1.1246 23.51 1.6874 29.062.38895 3.8372 3.8278 32.454 4.6105 38.459 4.6694-.24176 9.2946.2879 14.377 1.481 1.2359-3.2937 5.2496-13.088 8.886-21.623 6.249-14.668 8.4128-21.264 10.253-31.252 1.2464-6.7626 1.6341-12.156 1.4204-19.764-.36325-12.93-2.1234-19.487-6.9377-25.843-2.0833-2.7507-6.9865-7.6112-7.9127-7.8436-.79716-.20019-6.6946-1.0922-6.7755-1.0248-.02213.0182-5.0006-.41858-7.5248-.22808l-2.149-.22808h-3.3738z"/>
     <path style="fill:currentColor" d="m61.915 883.28-3.2484.4497c-1.7863.24724-3.5182.53481-3.8494.63994-2.4751.33811-4.7267.86957-6.7777 1.5696-.28598 0-1.0254.20146-2.3695.58589-5.0418 1.4418-6.6374 2.2604-8.2567 4.2364-6.281 7.6657-11.457 18.43-12.932 26.891-1.4667 8.4111.71353 22.583 5.0764 32.996 3.8064 9.0852 13.569 25.149 22.801 37.517 1.3741 1.841 2.1708 2.9286 2.4712 3.5792 3.5437-1.1699 6.8496-1.9336 10.082-2.3263-1.3569-5.7831-4.6968-21.86-6.8361-33.002-.92884-4.8368-2.4692-14.322-3.2452-19.991-.68557-5.0083-.77707-6.9534-.74159-15.791.04316-10.803.41822-16.162 1.5026-21.503 1.4593-5.9026 3.3494-11.077 6.3247-15.852z"/>
     <path style="fill:currentColor" d="m94.499 885.78c-.10214-.0109-.13691 0-.0907.0409.16033.13489 1.329 1.0675 2.5976 2.0723 6.7003 5.307 11.273 14.568 12.658 25.638.52519 4.1949.24765 14.361-.5059 18.523-2.4775 13.684-9.7807 32.345-20.944 53.519l-3.0559 5.7971c2.8082.76579 5.7915 1.727 8.9926 2.8441 11.562-11.691 18.349-19.678 24.129-28.394 7.8992-11.913 11.132-20.234 12.24-31.518.98442-10.02-1.5579-20.876-6.7799-28.959-.2758-.4269-.57803-.86856-.89617-1.3166-3.247-6.13-9.752-12.053-21.264-16.131-2.3687-.86369-6.3657-2.0433-7.0802-2.1166z"/>
     <path style="fill:currentColor" d="m32.52 892.22c-.20090-.13016-1.4606.81389-3.9132 2.7457-11.486 9.0476-17.632 24.186-16.078 39.61.79699 7.9138 2.4066 13.505 5.9184 20.562 5.8577 11.77 14.749 23.219 30.087 38.74.05838.059.12188.1244.18052.1838 1.3166-.5556 2.5965-1.0618 3.8429-1.5199-.66408-.32448-1.4608-1.3297-3.8116-4.4602-5.0951-6.785-8.7512-11.962-13.051-18.486-5.1379-7.7948-5.0097-7.5894-8.0586-13.054-6.2097-11.13-8.2674-17.725-8.6014-27.563-.21552-6.3494.13041-9.2733 1.775-14.987 2.1832-7.5849 3.9273-10.986 9.2693-18.07 1.7839-2.3656 2.6418-3.57 2.4409-3.7003z"/>
     <path style="fill:currentColor" d="m69.133 992.37c-6.2405.0309-12.635.76718-19.554 2.5706 4.6956 4.7759 9.935 10.258 12.05 12.625l4.1272 4.6202h11.493l3.964-4.4516c2.0962-2.3541 7.4804-7.9845 12.201-12.768-8.378-1.4975-16.207-2.6353-24.281-2.5955z"/>
     <rect style="stroke-width:0;fill:currentColor" ry="2.0328" height="27.746" width="22.766" y="1017.7" x="60.201"/>
    </g>
   </g>
  </symbol>

  <symbol id="info" viewBox="0 0 41.667 41.667">
   <g transform="translate(-37.035 -1004.6)">
    <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m76.25 1030.2a18.968 18.968 0 0 1 -23.037 13.709 18.968 18.968 0 0 1 -13.738 -23.019 18.968 18.968 0 0 1 23.001 -13.768 18.968 18.968 0 0 1 13.798 22.984"/>
    <g transform="matrix(1.1146 0 0 1.1146 -26.276 -124.92)">
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m75.491 1039.5v-8.7472"/>
     <path style="stroke-width:0;fill:currentColor" transform="scale(-1)" d="m-73.193-1024.5a2.3719 2.3719 0 0 1 -2.8807 1.7142 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
   </g>
  </symbol>

  <symbol id="warning" viewBox="0 0 48.430474 41.646302">
    <g transform="translate(-1.1273 -1010.2)">
     <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:4.151;fill:none" d="m25.343 1012.3-22.14 37.496h44.28z"/>
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:4.1512;fill:none" d="m25.54 1027.7v8.7472"/>
     <path style="stroke-width:0;fill:currentColor" d="m27.839 1042.8a2.3719 2.3719 0 0 1 -2.8807 1.7143 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
  </symbol>

  <symbol id="menu" viewBox="0 0 50 50">
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="0" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="20" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="40" x="0"/>
   </symbol>

   <symbol id="link" viewBox="0 0 50 50">
    <g transform="translate(0 -1002.4)">
     <g transform="matrix(.095670 0 0 .095670 2.3233 1004.9)">
      <g>
       <path style="stroke-width:0;fill:currentColor" d="m452.84 192.9-128.65 128.65c-35.535 35.54-93.108 35.54-128.65 0l-42.881-42.886 42.881-42.876 42.884 42.876c11.845 11.822 31.064 11.846 42.886 0l128.64-128.64c11.816-11.831 11.816-31.066 0-42.9l-42.881-42.881c-11.822-11.814-31.064-11.814-42.887 0l-45.928 45.936c-21.292-12.531-45.491-17.905-69.449-16.291l72.501-72.526c35.535-35.521 93.136-35.521 128.64 0l42.886 42.881c35.535 35.523 35.535 93.141-.001 128.66zm-254.28 168.51-45.903 45.9c-11.845 11.846-31.064 11.817-42.881 0l-42.884-42.881c-11.845-11.821-11.845-31.041 0-42.886l128.65-128.65c11.819-11.814 31.069-11.814 42.884 0l42.886 42.886 42.876-42.886-42.876-42.881c-35.54-35.521-93.113-35.521-128.65 0l-128.65 128.64c-35.538 35.545-35.538 93.146 0 128.65l42.883 42.882c35.51 35.54 93.11 35.54 128.65 0l72.496-72.499c-23.956 1.597-48.092-3.784-69.474-16.283z"/>
      </g>
     </g>
    </g>
  </symbol>

  <symbol id="doc" viewBox="0 0 35 45">
   <g transform="translate(-147.53 -539.83)">
    <path style="stroke:currentColor;stroke-width:2.4501;fill:none" d="m149.38 542.67v39.194h31.354v-39.194z"/>
    <g style="stroke-width:25" transform="matrix(.098003 0 0 .098003 133.69 525.96)">
     <path d="m220 252.36h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path style="stroke:currentColor;stroke-width:25;fill:none" d="m220 409.95h200"/>
     <path d="m220 488.74h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path d="m220 331.15h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
    </g>
   </g>
 </symbol>

 <symbol id="tick" viewBox="0 0 177.16535 177.16535">
  <g transform="translate(0 -875.2)">
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="155" width="40" y="702.99" x="556.82"/>
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="40" width="90.404" y="817.99" x="506.42"/>
  </g>
 </symbol>
</svg>

    <div class="wrapper">
      <header class="intro-and-nav" role="banner">
  <div>
    <div class="intro">
      <a class="logo" href="/" aria-label="Kevin Davenport Engineering &amp; ML blog home page">
        <img src="https://kldavenport.com/images/logo.svg" alt="">
      </a>
      <p class="library-desc">
        
      </p>
    </div>
    <nav id="patterns-nav" class="patterns" role="navigation">
  <h2 class="vh">Main navigation</h2>
  <button id="menu-button" aria-expanded="false">
    <svg viewBox="0 0 50 50" aria-hidden="true" focusable="false">
      <use xlink:href="#menu"></use>
    </svg>
    Menu
  </button>
  
  <ul id="patterns-list">
  
    <li class="pattern">
      
      
      
      
      <a href="/post/" aria-current="page">
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Blog</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/about/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">About</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/bike-travel/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Bike Travel</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/tags/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Tags</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/index.xml" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">RSS</span>
      </a>
    </li>
  
  </ul>
</nav>
  </div>
</header>
      <div class="main-and-footer">
        <div>
          
  <main id="main">
    <h1>
      <svg class="bookmark-icon" aria-hidden="true" viewBox="0 0 40 50" focusable="false">
        <use xlink:href="#bookmark"></use>
      </svg>
      Topic Modeling Amazon Product Reviews
    </h1>
    <div class="date">
      
      <strong aria-hidden="true">Publish date: </strong>Mar 17, 2017
    </div>
    
      <div class="tags">
        <strong aria-hidden="true">Tags: </strong>
        <ul aria-label="tags">
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.com/tags/gensim">Gensim</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.com/tags/nltk">NLTK</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.com/tags/lda">LDA</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.com/tags/nlp">NLP</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.com/tags/nlu">NLU</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              <a href="https://kldavenport.com/tags/topic-modeling">Topic Modeling</a>
            </li>
          
        </ul>
      </div>
    
    
    
      

  <nav class="toc" aria-labelledby="toc-heading">
    <h2 id="toc-heading">Table of contents</h2>
    <ol>
      
        <li>
          <a href="#loading-our-data">
            Loading our data
          </a>
        </li>
      
        <li>
          <a href="#tokenization">
            Tokenization
          </a>
        </li>
      
        <li>
          <a href="#stop-words">
            Stop Words
          </a>
        </li>
      
        <li>
          <a href="#stemming">
            Stemming
          </a>
        </li>
      
        <li>
          <a href="#putting-together-a-document-term-matrix">
            Putting together a document-term matrix
          </a>
        </li>
      
        <li>
          <a href="#transform-tokenized-documents-into-an-id-term-dictionary">
            Transform tokenized documents into an id-term dictionary
          </a>
        </li>
      
        <li>
          <a href="#creating-bag-of-words">
            Creating bag of words
          </a>
        </li>
      
        <li>
          <a href="#training-an-lda-model">
            Training an LDA model
          </a>
        </li>
      
        <li>
          <a href="#inferring-topics">
            Inferring Topics
          </a>
        </li>
      
        <li>
          <a href="#querying-the-lda-model">
            Querying the LDA Model
          </a>
        </li>
      
        <li>
          <a href="#what-can-we-do-with-this-in-production">
            What can we do with this in production?
          </a>
        </li>
      
    </ol>
  </nav>


    
    

<p>I found Professor Julian McAuley’s <a href="https://cseweb.ucsd.edu/~jmcauley/">work at UCSD</a> when I was searching for academic work identifying the ontology and utility of products on Amazon. Professor McAuley and his students have accomplished impressive work inferring networks of substitutable and complementary items. They constructed a browseable product graph of related products and discovered topics or ‘microcategories’ that are associated with product relationships to infer networks of substitutable and complementary products. Much of this work utilizes topic modeling, and as I’ve never applied it in academia or work, this blog will be a practical intro to <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet Allocation (LDA)</a> through code. More broadly what can we do with and what do we need to know about LDA?</p>

<ul>
<li>It is an Unsupervised Learning Technique that assumes documents are produced from a mixture of topics</li>
<li>LDA extracts key topics and themes from a large corpus of text</li>
<li>Each topic is a ordered list of representative words (order is based on importance of word to a topic)</li>
<li>LDA describes each document in the corpus based on allocation to the extracted topics</li>
<li>Many domain specific methods to create training datasets</li>
<li>It is easy to use for exploratory analysis</li>
</ul>

<p><img src="image0.png" alt="png" /></p>

<p>We’ll be using a subset <code>(reviews_Automotive_5.json.gz)</code> of the 142.8 million reviews spanning May 1996 – July 2014 that Julian and his team have compiled and provided in a very convenient manner on their site.</p>

<pre><code class="language-python">from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from stop_words import get_stop_words
from nltk.stem.snowball import SnowballStemmer
from gensim import corpora, models
import gensim
</code></pre>

<h2 id="loading-our-data">Loading our data</h2>

<p>Warning: you will receive an error message when trying to use nltk&rsquo;s stopwords if you don&rsquo;t explicitly download the stopwords first:</p>

<pre><code class="language-python">import nltk
nltk.download(&quot;stopwords&quot;)
</code></pre>

<p>Loading the provided reviews subset JSON into a Pandas dataframe:</p>

<pre><code class="language-python">import pandas as pd
import gzip

# one-review-per-line in json
def parse(path):
    g = gzip.open(path, 'rb')
    for l in g:
        yield eval(l)

def getDF(path):
    i = 0
    df = {}
    for d in parse(path):
        df[i] = d
        i += 1
    return pd.DataFrame.from_dict(df, orient='index')

df = getDF('reviews_Automotive_5.json.gz')

df.head()
</code></pre>

<p><img src="table1.png" alt="png" /></p>

<pre><code class="language-python">df['reviewText'][0]
</code></pre>

<p><code>&quot;I neded a set of jumper cables for my new car and these had good reviews and were at a good price.  They have been used a few times already and do what they are supposed to - no complaints there.What I will say is that 12 feet really isn't an ideal length.  Sure, if you pull up front bumper to front bumper they are plenty long, but a lot of times you will be beside another car or can't get really close.  Because of this, I would recommend something a little longer than 12'.Great brand - get 16' version though.&quot;</code></p>

<pre><code class="language-python">df.info()
</code></pre>

<pre><code>Int64Index: 20473 entries, 0 to 20472
Data columns (total 9 columns):
reviewerID        20473 non-null object
asin              20473 non-null object
reviewerName      20260 non-null object
helpful           20473 non-null object
unixReviewTime    20473 non-null int64
reviewText        20473 non-null object
overall           20473 non-null float64
reviewTime        20473 non-null object
summary           20473 non-null object
dtypes: float64(1), int64(1), object(7)
memory usage: 1.6+ MB
</code></pre>

<p>Now that we have a nice corpus of text, lets go through some of the standard preprocessing required for almost any topic modeling or NLP problem.</p>

<p>Our Approach will involve:</p>

<ul>
<li>Tokenizing: converting a document to its atomic elements</li>
<li>Stopping: removing meaningless words</li>
<li>Stemming: merging words that are equivalent in meaning</li>
</ul>

<h2 id="tokenization">Tokenization</h2>

<p>We have many ways to segment our document into its atomic elements. To start we&rsquo;ll tokenize the document into words. For this instance we&rsquo;ll use NLTK’s tokenize.regexp module. You can see how this works in a fun interactive way here: try &lsquo;w+&rsquo; at <a href="http://regexr.com/:">http://regexr.com/:</a></p>

<p><img src="image1.gif" alt="gif" /></p>

<pre><code class="language-python">tokenizer = RegexpTokenizer(r'\w+')
</code></pre>

<p>Running through part of the first review to demonstrate:</p>

<pre><code class="language-python">doc_1 = df.reviewText[0]

# Using one of our docs as an example
tokens = tokenizer.tokenize(doc_1.lower())

print('{} characters in string vs {} words in a list'.format(len(doc_1),                                                             len(tokens)))
print(tokens[:10])
</code></pre>

<p><code>516 characters in string vs 103 words in a list ['i', 'needed', 'a', 'set', 'of', 'jumper', 'cables', 'for', 'my', 'new']</code></p>

<h2 id="stop-words">Stop Words</h2>

<p>Determiners like &ldquo;the&rdquo; and conjunctions such as &ldquo;or&rdquo; and &ldquo;for&rdquo; do not add value to our simple topic model. We refer to these types of words as stop words and want to remove them from our list of tokens. The definition of a stop work changes depending on the context of the documents we are examining. If considering Product Reviews for <a href="https://www.amazon.com/s/ref=nb_sb_ss_c_0_18?url=search-alias%3Dtoys-and-games&amp;field-keywords=childrens+board+games&amp;sprefix=childrens+board+games%2Caps%2C200&amp;rh=n%3A165793011%2Ck%3Achildrens+board+games">children&rsquo;s board games on Amazon.com</a> we would not find &ldquo;Chutes and Ladders&rdquo; as a token and eventually an entity in some other model if we remove the word &ldquo;and&rdquo; as we&rsquo;ll end up with a distinct &ldquo;chutes&rdquo; AND &ldquo;ladders&rdquo; in our list.</p>

<p>Let&rsquo;s make a super list of stop words from the stop_words and nltk package below. By the way if you&rsquo;re using Python 3 you can make use of an odd new feature to unpack lists into a new list:</p>

<pre><code class="language-python">merged_stopwords = [*nltk_stpwd, *stop_words_stpwd] # Python 3 oddity insanity to merge lists
</code></pre>

<p>Back to python 2.6</p>

<pre><code class="language-python">nltk_stpwd = stopwords.words('english')
stop_words_stpwd = get_stop_words('en')
merged_stopwords = list(set(nltk_stpwd + stop_words_stpwd))

print(len(set(merged_stopwords)))
print(merged_stopwords[:10])
</code></pre>

<p><code>207</code></p>

<p><code>[u'all', u&quot;she'll&quot;, u'just', u&quot;don't&quot;, u'being', u'over', u'both', u'through', u'yourselves', u'its']</code></p>

<pre><code class="language-python">stopped_tokens = [token for token in tokens if not token in merged_stopwords]
print(stopped_tokens[:10])
</code></pre>

<p><code>['needed', 'set', 'jumper', 'cables', 'new', 'car', 'good', 'reviews', 'good', 'price']</code></p>

<h2 id="stemming">Stemming</h2>

<p>Stemming allows us to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance, running and runner to run. Another example:</p>

<p>Amazon&rsquo;s catalog contains bike tires in different sizes and colors $\Rightarrow$ Amazon catalog contain bike tire in differ size and color</p>

<p>Stemming is a basic and crude heuristic compared to Lemmatization which understands vocabulary and morphological analysis instead of lobbing off the end of words. Essentially Lemmatization removes inflectional endings to return the word to its base or dictionary form of a word, which is defined as the lemma. Great illustrative examples from Wikipedia:</p>

<ul>
<li>The word &ldquo;better&rdquo; has &ldquo;good&rdquo; as its lemma. This link is missed by stemming, as it requires a dictionary look-up.</li>
<li>The word &ldquo;walk&rdquo; is the base form for word &ldquo;walking&rdquo;, and hence this is matched in both stemming and lemmatisation.</li>
<li>The word &ldquo;meeting&rdquo; can be either the base form of a noun or a form of a verb (&ldquo;to meet&rdquo;) depending on the context, e.g., &ldquo;in our last meeting&rdquo; or &ldquo;We are meeting again tomorrow&rdquo;. Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context.</li>
</ul>

<p>We&rsquo;ll start with the common <a href="https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg">Snowball stemming method</a>, a successor of sorts of the original Porter Stemmer which is implemented in NLTK:</p>

<pre><code class="language-python"># Instantiate a Snowball stemmer
sb_stemmer = SnowballStemmer('english')
</code></pre>

<p>Note that p_stemmer requires all tokens to be type str. p_stemmer returns the string parameter in stemmed form, so we need to loop through our stopped_tokens:</p>

<pre><code class="language-python">stemmed_tokens = [sb_stemmer.stem(token) for token in stopped_tokens]
print(stemmed_tokens)
</code></pre>

<p><code>[u'need', u'set', u'jumper', u'cabl', u'new', u'car', u'good', u'review', u'good', u'price', u'use', u'time', u'alreadi', u'suppos', u'complaint', u'say', '12', u'feet', u'realli', u'ideal', u'length', u'sure', u'pull', u'front', u'bumper', u'front', u'bumper', u'plenti', u'long', u'lot', u'time', u'besid', u'anoth', u'car', u'get', u'realli', u'close', u'recommend', u'someth', u'littl', u'longer', '12', u'great', u'brand', u'get', '16', u'version', u'though']</code></p>

<h2 id="putting-together-a-document-term-matrix">Putting together a document-term matrix</h2>

<p>In order to create an LDA model we&rsquo;ll need to put the 3 steps from above (tokenizing, stopping, stemming) together to create a list of documents (list of lists) to then generate a document-term matrix (unique terms as rows, documents or reviews as columns). This matrix will tell us how frequently each term occurs with each individual document.</p>

<pre><code class="language-python">%%time

num_reviews = df.shape[0]

doc_set = [df.reviewText[i] for i in range(num_reviews)]

texts = []

for doc in doc_set:
    # putting our three steps together
    tokens = tokenizer.tokenize(doc.lower())
    stopped_tokens = [token for token in tokens if not token in merged_stopwords]
    stemmed_tokens = [sb_stemmer.stem(token) for token in stopped_tokens]
    
    # add tokens to list
    texts.append(stemmed_tokens)
</code></pre>

<p><code>CPU times: user 36.6 s, sys: 263 ms, total: 36.9 s</code>
<code>Wall time: 36.9 s</code></p>

<pre><code class="language-python">print texts[0] # examine review 1
</code></pre>

<p><code>[u'need', u'set', u'jumper', u'cabl', u'new', u'car', u'good', u'review', u'good', u'price', u'use', u'time', u'alreadi', u'suppos', u'complaint', u'say', '12', u'feet', u'realli', u'ideal', u'length', u'sure', u'pull', u'front', u'bumper', u'front', u'bumper', u'plenti', u'long', u'lot', u'time', u'besid', u'anoth', u'car', u'get', u'realli', u'close', u'recommend', u'someth', u'littl', u'longer', '12', u'great', u'brand', u'get', '16', u'version', u'though']</code></p>

<h2 id="transform-tokenized-documents-into-an-id-term-dictionary">Transform tokenized documents into an id-term dictionary</h2>

<p>Gensim&rsquo;s Dictionary method encapsulates the mapping between normalized words and their integer ids. Note a term will have an id of some number and in the subsequent bag of words step we can see that id will have a count associated with it.</p>

<pre><code class="language-python"># Gensim's Dictionary encapsulates the mapping between normalized words and their integer ids.
texts_dict = corpora.Dictionary(texts)
texts_dict.save('auto_review.dict') # lets save to disk for later use
# Examine each token’s unique id
print(texts_dict)
</code></pre>

<p><code>Dictionary(19216 unique tokens: [u'circuitri', u'html4', u'pathfind', u'spoonssmal', u'suspend']...)</code></p>

<p>To see the mapping between words and their ids we can use the token2id method:</p>

<pre><code class="language-python">import operator
print(&quot;IDs 1 through 10: {}&quot;.format(sorted(texts_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:10]))
</code></pre>

<p><code>IDs 1 through 10: [(u'set', 0), (u'cabl', 1), (u'realli', 2), (u'feet', 3), (u'say', 4), (u'alreadi', 5), (u'long', 6), (u'need', 7), (u'close', 8), (u'use', 9)]</code></p>

<p>Let&rsquo;s try to guess the original work and examine the count difference between our #1 most frequent term and our #10 most frequent term:</p>

<pre><code class="language-python">print(df.reviewText.str.contains(&quot;complaint&quot;).value_counts()) 
print(df.reviewText.str.contains(&quot;lot&quot;).value_counts())
</code></pre>

<p>We have a lot of unique tokens, let&rsquo;s see what happens if we ignore tokens that appear in less than 30 documents or more than 15% documents. Granted this is arbitrary but a quick search shows tons of methods for reducing noise.</p>

<pre><code class="language-python">texts_dict.filter_extremes(no_below=30, no_above=0.15) # inlace filter
print(texts_dict)
print(&quot;top terms:&quot;)
print(sorted(texts_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:10])
</code></pre>

<p><code>Dictionary(2464 unique tokens: [u'saver', u'yellow', u'hitch', u'four', u'sleev']...)</code></p>

<p><code>top terms:
[(u'saver', 0), (u'yellow', 1), (u'hitch', 2), (u'four', 3), (u'sleev', 4), (u'upsid', 5), (u'hate', 6), (u'forget', 7), (u'accur', 8), (u'sorri', 9)]</code></p>

<p>We went from <strong>19216</strong> unique tokens to <strong>2462</strong> after filtering. Looking at the top 10 tokens it looks like we got more specific subjects opposed to adjectives.</p>

<h2 id="creating-bag-of-words">Creating bag of words</h2>

<p>Next let&rsquo;s turn texts_dict into a bag of words instead. doc2bow converts a document (a list of words) into the bag-of-words format (list of (token_id, token_count) tuples).</p>

<pre><code class="language-python">corpus = [texts_dict.doc2bow(text) for text in texts]
len(corpus)
</code></pre>

<p><code>20473</code></p>

<p>The corpus is 20473 long, the amount of reviews in our dataset and in our dataframe. Let&rsquo;s dump this bag-of-words into a file to avoid parsing the entire text again:</p>

<pre><code class="language-python">%%time 
# Matrix Market format https://radimrehurek.com/gensim/corpora/mmcorpus.html, why exactly? I don't know
gensim.corpora.MmCorpus.serialize('amzn_auto_review.mm', corpus)
</code></pre>

<p><code>CPU times: user 2.34 s, sys: 70.2 ms, total: 2.42 s</code></p>

<p><code>Wall time: 2.42 s</code></p>

<h2 id="training-an-lda-model">Training an LDA model</h2>

<p>As a topic modeling newbie this part is unsatisfying to me. In this unsupervised learning application I can see how a lot of people would arbitrarily set a number of topics, similar to centroids in k-means clustering, and then have a human evaluate if the topics &ldquo;make sense&rdquo;. You can go very deep very quickly by researching this online. For now let&rsquo;s plead ignorance and go through with a simple model FULL of assumptions :)</p>

<p>Training an LDA model using our BOW corpus as training data:
<img src="image2.jpeg" alt="jpeg" /></p>

<p>The number of topics is arbitrary, I&rsquo;ll use the browse taxonomy visible off <a href="https://www.amazon.com/automotive">https://www.amazon.com/automotive</a> to inform the number we choose:</p>

<ol>
<li>Performance Parts &amp; Accessories</li>
<li>Replacement Parts</li>
<li>Truck Accessories</li>
<li>Interior Accessories</li>
<li>Exterior Accessories</li>
<li>Tires &amp; Wheels</li>
<li>Car Care</li>
<li>Tools &amp; Equipment</li>
<li>Motorcycle &amp; Powersports Accessories</li>
<li>Car Electronics</li>
<li>Enthusiast Merchandise</li>
</ol>

<p>I think these categories could be compressed into 5 general topics. We might consider rolling #9 into 4 &amp; 5, and rolling the products in #3 across other accessory categories and so on.</p>

<pre><code class="language-python">%%time 
lda_model = gensim.models.LdaModel(corpus,alpha='auto', num_topics=5,id2word=texts_dict, passes=20)
# ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = texts_dict, passes=20)
</code></pre>

<p><code>CPU times: user 6min 24s, sys: 2.7 s, total: 6min 27s</code></p>

<p><code>Wall time: 6min 28s</code></p>

<p>Note: Gensim offers a fantastic multicore implementation of LDAModel that reduced my training time by 75%, but it does not have the auto alpha parameter available. Exploring hyperparameter tuning will go beyond the high-level of this post. See here for a great resource: <a href="http://stats.stackexchange.com/questions/37405/natural-interpretation-for-lda-hyperparameters">http://stats.stackexchange.com/questions/37405/natural-interpretation-for-lda-hyperparameters</a></p>

<h2 id="inferring-topics">Inferring Topics</h2>

<p>Below are the top 5 words associated with 5 random topics. The float next to each word is the weight showing how much the given word influences this specific topic. In this case, we see that for topic 4, light and battery are the most telling words. We might interpret that topic 4 might be close to Amazon&rsquo;s Tools &amp; Equipment category which has a sub-category titled &ldquo;Jump Starters, Battery Chargers &amp; Portable Power&rdquo;. Similarly we might infer topic 1 refers to Car Care, maybe sub category &ldquo;Exterior Care&rdquo;.</p>

<pre><code class="language-python"># For `num_topics` number of topics, return `num_words` most significant words
lda_model.show_topics(num_topics=5,num_words=5)
</code></pre>

<p><code>[(0,
  u'0.031*&quot;blade&quot; + 0.025*&quot;wiper&quot; + 0.017*&quot;hose&quot; + 0.016*&quot;water&quot; + 0.012*&quot;windshield&quot;'),
 (1,
  u'0.017*&quot;towel&quot; + 0.016*&quot;clean&quot; + 0.013*&quot;wash&quot; + 0.013*&quot;dri&quot; + 0.010*&quot;wax&quot;'),
 (2,
  u'0.010*&quot;fit&quot; + 0.009*&quot;tire&quot; + 0.007*&quot;instal&quot; + 0.006*&quot;nice&quot; + 0.006*&quot;back&quot;'),
 (3,
  u'0.013*&quot;oil&quot; + 0.011*&quot;drive&quot; + 0.011*&quot;filter&quot; + 0.009*&quot;engin&quot; + 0.008*&quot;app&quot;'),
 (4,
  u'0.033*&quot;light&quot; + 0.024*&quot;batteri&quot; + 0.014*&quot;power&quot; + 0.013*&quot;charg&quot; + 0.012*&quot;bulb&quot;')]</code></p>

<p>Note that LDA is a probabilistic mixture of mixtures (or admixture) model for grouped data. The observed data (words) within the groups (documents) are the result of probabilistically choosing words from a specific topic (multinomial over the vocabulary), where the topic is itself drawn from a document-specific multinomial that has a global Dirichlet prior. This means that words can belong to various topics in various degrees. For example, the word &lsquo;pressure&rsquo; might refer to a category/topic of automotive wash products and a category of tire products (in the case where we think the topics are about classes of products).</p>

<h2 id="querying-the-lda-model">Querying the LDA Model</h2>

<p>We cannot pass an arbitrary string to our model and evaluate what topics are most associated with it.</p>

<pre><code class="language-python">raw_query = 'portable air compressor'

query_words = raw_query.split()
query = []
for word in query_words:
    # ad-hoc reuse steps from above
    q_tokens = tokenizer.tokenize(word.lower())
    q_stopped_tokens = [word for word in q_tokens if not word in merged_stopwords]
    q_stemmed_tokens = [sb_stemmer.stem(word) for word in q_stopped_tokens]
    query.append(q_stemmed_tokens[0]) # different frome above, this is not a lists of lists!
    
print query
</code></pre>

<p><code>[u'portabl', u'air', u'compressor']</code></p>

<pre><code class="language-python"># translate words in query to ids and frequencies. 
id2word = gensim.corpora.Dictionary()
_ = id2word.merge_with(texts_dict) # garbage

# translate this document into (word, frequency) pairs
query = id2word.doc2bow(query)
print(query)
</code></pre>

<p><code>[(53, 1), (238, 1), (408, 1)]</code></p>

<p>If we run this constructed query against our trained mode we will get each topic and the likelihood that the query relates to that topic. Remember we arbitrarily specified 11 topics when we made the model. When we organize this list to find the most relative topics, we see some intuitive results. We see that our query of &lsquo;battery powered inflator&rsquo; relates most to a topic we thought might align to Amazon&rsquo;s Tools &amp; Equipment category which has a sub-category titled &ldquo;Jump Starters, Battery Chargers &amp; Portable Power&rdquo;.</p>

<pre><code class="language-python">a = list(sorted(lda_model[query], key=lambda x: x[1])) # sort by the second entry in the tuple
a
</code></pre>

<p><code>[(0, 0.017966903726103274),
 (3, 0.027522816624803454),
 (1, 0.029587736744938701),
 (2, 0.049382812891815127),
 (4, 0.87553973001233942)]</code></p>

<pre><code class="language-python">lda_model.print_topic(a[0][0]) #least related
</code></pre>

<p><code>u'0.031*&quot;blade&quot; + 0.025*&quot;wiper&quot; + 0.017*&quot;hose&quot; + 0.016*&quot;water&quot; + 0.012*&quot;windshield&quot; + 0.010*&quot;mat&quot; + 0.010*&quot;instal&quot; + 0.009*&quot;rain&quot; + 0.008*&quot;fit&quot; + 0.008*&quot;tank&quot;'</code></p>

<pre><code class="language-python">lda_model.print_topic(a[-1][0]) #most related
</code></pre>

<p><code>u'0.033*&quot;light&quot; + 0.024*&quot;batteri&quot; + 0.014*&quot;power&quot; + 0.013*&quot;charg&quot; + 0.012*&quot;bulb&quot; + 0.009*&quot;led&quot; + 0.008*&quot;plug&quot; + 0.008*&quot;bright&quot; + 0.008*&quot;phone&quot; + 0.008*&quot;connect&quot;'</code></p>

<h2 id="what-can-we-do-with-this-in-production">What can we do with this in production?</h2>

<p>We could take these inferred topics and analyze the sentiment of their corresponding documents (reviews) to find out what customers are saying (or feeling) about specific products. We can also use an LDA model to extract representative statements or quotes, enabling us to summarize customers’ opinions about products, perhaps even displaying them on the site.We could also use LDA to model groups of customers to topics which are groups of products that frequently occur within some customer&rsquo;s orders over time.</p>

  </main>

          <footer role="contentinfo">
  <div>
    <label for="themer">
      dark theme: <input type="checkbox" id="themer" class="vh">
      <span aria-hidden="true"></span>
    </label>
  </div>
  
    Made w/ <a href="https://gohugo.io/">Hugo</a>. Deployed by <a href="https://app.netlify.com/">Netlify</a>.
  
</footer>

        </div>
      </div>
    </div>
    <script src="https://kldavenport.com/js/prism.js"></script>
<script src="https://kldavenport.com/js/dom-scripts.js"></script>

    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-34706513-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>
